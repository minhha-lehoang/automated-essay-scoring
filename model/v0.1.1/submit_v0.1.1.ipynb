{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:53:17.357326Z","iopub.status.busy":"2024-05-14T15:53:17.357055Z","iopub.status.idle":"2024-05-14T15:53:29.805965Z","shell.execute_reply":"2024-05-14T15:53:29.805042Z","shell.execute_reply.started":"2024-05-14T15:53:17.357301Z"},"papermill":{"duration":11.069288,"end_time":"2024-05-06T18:34:16.236621","exception":false,"start_time":"2024-05-06T18:34:05.167333","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import random\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from string import punctuation, printable\n","import re\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","\n","import sklearn\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import LongformerModel, LongformerTokenizer\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","random_seed = 42\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed(random_seed)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:53:29.808153Z","iopub.status.busy":"2024-05-14T15:53:29.807670Z","iopub.status.idle":"2024-05-14T15:54:08.543140Z","shell.execute_reply":"2024-05-14T15:54:08.542146Z","shell.execute_reply.started":"2024-05-14T15:53:29.808126Z"},"papermill":{"duration":37.397163,"end_time":"2024-05-06T18:34:53.638204","exception":false,"start_time":"2024-05-06T18:34:16.241041","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing ./pyspellchecker-0.8.1-py3-none-any.whl\n","Installing collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.8.1\n"]}],"source":["from distutils.dir_util import copy_tree\n","\n","copy_tree('/kaggle/input/spellchecker', '/kaggle/working/')\n","\n","%gzip '/kaggle/working/spellchecker/resources/en.json'\n","\n","%pip install '/kaggle/working/pyspellchecker-0.8.1-py3-none-any.whl'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:54:08.545128Z","iopub.status.busy":"2024-05-14T15:54:08.544736Z","iopub.status.idle":"2024-05-14T15:54:08.741509Z","shell.execute_reply":"2024-05-14T15:54:08.740612Z","shell.execute_reply.started":"2024-05-14T15:54:08.545088Z"},"papermill":{"duration":0.203142,"end_time":"2024-05-06T18:34:53.846134","exception":false,"start_time":"2024-05-06T18:34:53.642992","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from spellchecker import SpellChecker\n","\n","spell = SpellChecker()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:54:08.743726Z","iopub.status.busy":"2024-05-14T15:54:08.743427Z","iopub.status.idle":"2024-05-14T15:54:08.769626Z","shell.execute_reply":"2024-05-14T15:54:08.768778Z","shell.execute_reply.started":"2024-05-14T15:54:08.743700Z"},"papermill":{"duration":0.023856,"end_time":"2024-05-06T18:34:53.874826","exception":false,"start_time":"2024-05-06T18:34:53.850970","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(3, 2)\n"]},{"data":{"text/plain":["['mean_num_verbs_in_paragraph',\n"," 'mean_num_pronouns_in_paragraph',\n"," 'num_words_in_essay',\n"," 'mean_num_verbs_in_sentence',\n"," 'max_num_pronouns_in_paragraph',\n"," 'max_num_proper_nouns_in_sentence',\n"," '25th_percentile_num_sentences_in_paragraph',\n"," 'min_num_adverbs_in_paragraph',\n"," 'max_num_conjunctions_in_sentence',\n"," '75th_percentile_num_misspelled_words_in_sentence',\n"," 'mean_num_adjectives_in_paragraph',\n"," '25th_percentile_num_proper_nouns_in_paragraph',\n"," 'mean_word_lens_in_essay',\n"," '25th_percentile_num_words_in_sentence',\n"," '75th_percentile_mean_word_lens_in_sentence',\n"," 'min_num_conjunctions_in_sentence',\n"," 'max_num_words_in_sentence',\n"," '25th_percentile_mean_word_lens_in_sentence',\n"," 'max_mean_word_lens_in_sentence',\n"," 'max_num_adverbs_in_sentence',\n"," '25th_percentile_num_words_in_paragraph',\n"," '75th_percentile_num_conjunctions_in_paragraph',\n"," 'min_num_nouns_in_sentence',\n"," '75th_percentile_num_nouns_in_paragraph',\n"," 'mean_num_sentences_in_paragraph',\n"," '25th_percentile_num_conjunctions_in_sentence',\n"," 'mean_num_words_in_paragraph',\n"," 'max_num_words_in_paragraph',\n"," 'min_num_adjectives_in_sentence',\n"," 'num_conjunctions_in_essay',\n"," '75th_percentile_num_pronouns_in_paragraph',\n"," '25th_percentile_num_adjectives_in_paragraph',\n"," 'min_num_misspelled_words_in_paragraph',\n"," 'min_num_conjunctions_in_paragraph',\n"," '75th_percentile_num_proper_nouns_in_sentence',\n"," 'num_adjectives_in_essay',\n"," '25th_percentile_num_conjunctions_in_paragraph',\n"," '25th_percentile_num_adjectives_in_sentence',\n"," '75th_percentile_num_adjectives_in_sentence',\n"," 'min_num_adverbs_in_sentence',\n"," 'min_num_proper_nouns_in_paragraph',\n"," '25th_percentile_num_proper_nouns_in_sentence',\n"," 'max_num_misspelled_words_in_sentence',\n"," 'mean_num_misspelled_words_in_paragraph',\n"," '25th_percentile_num_adverbs_in_paragraph',\n"," 'num_verbs_in_essay',\n"," 'mean_mean_word_lens_in_paragraph',\n"," 'num_proper_nouns_in_essay',\n"," 'min_num_verbs_in_sentence',\n"," 'num_adverbs_in_essay',\n"," '25th_percentile_num_pronouns_in_paragraph',\n"," '75th_percentile_num_words_in_paragraph',\n"," '25th_percentile_num_misspelled_words_in_sentence',\n"," 'max_num_nouns_in_sentence',\n"," 'mean_num_nouns_in_sentence',\n"," 'max_num_proper_nouns_in_paragraph',\n"," '25th_percentile_num_verbs_in_sentence',\n"," 'max_num_adjectives_in_sentence',\n"," 'min_num_sentences_in_paragraph',\n"," 'min_num_pronouns_in_sentence',\n"," '75th_percentile_num_pronouns_in_sentence',\n"," 'min_num_words_in_sentence',\n"," '75th_percentile_num_misspelled_words_in_paragraph',\n"," '75th_percentile_num_conjunctions_in_sentence',\n"," '75th_percentile_num_verbs_in_sentence',\n"," '75th_percentile_num_nouns_in_sentence',\n"," 'max_num_nouns_in_paragraph',\n"," 'max_mean_word_lens_in_paragraph',\n"," '75th_percentile_num_sentences_in_paragraph',\n"," 'mean_num_adverbs_in_sentence',\n"," 'max_num_verbs_in_paragraph',\n"," '25th_percentile_num_misspelled_words_in_paragraph',\n"," 'min_num_nouns_in_paragraph',\n"," 'max_num_verbs_in_sentence',\n"," 'min_mean_word_lens_in_sentence',\n"," 'max_num_conjunctions_in_paragraph',\n"," 'min_num_verbs_in_paragraph',\n"," '75th_percentile_mean_word_lens_in_paragraph',\n"," '25th_percentile_mean_word_lens_in_paragraph',\n"," 'mean_mean_word_lens_in_sentence',\n"," '25th_percentile_num_nouns_in_paragraph',\n"," '75th_percentile_num_verbs_in_paragraph',\n"," 'mean_num_conjunctions_in_paragraph',\n"," 'mean_num_words_in_sentence',\n"," 'min_num_words_in_paragraph',\n"," 'max_num_pronouns_in_sentence',\n"," '75th_percentile_num_words_in_sentence',\n"," 'mean_num_proper_nouns_in_sentence',\n"," 'max_num_adjectives_in_paragraph',\n"," 'mean_num_pronouns_in_sentence',\n"," 'mean_num_adjectives_in_sentence',\n"," '75th_percentile_num_proper_nouns_in_paragraph',\n"," 'min_mean_word_lens_in_paragraph',\n"," 'mean_num_proper_nouns_in_paragraph',\n"," 'mean_num_nouns_in_paragraph',\n"," 'num_pronouns_in_essay',\n"," 'num_paragraphs',\n"," 'max_num_misspelled_words_in_paragraph',\n"," 'mean_num_conjunctions_in_sentence',\n"," 'min_num_misspelled_words_in_sentence',\n"," '25th_percentile_num_nouns_in_sentence',\n"," 'mean_num_adverbs_in_paragraph',\n"," '75th_percentile_num_adverbs_in_paragraph',\n"," 'max_num_sentences_in_paragraph',\n"," '25th_percentile_num_adverbs_in_sentence',\n"," '25th_percentile_num_pronouns_in_sentence',\n"," 'min_num_proper_nouns_in_sentence',\n"," 'num_misspelled_words_in_essay',\n"," 'mean_num_misspelled_words_in_sentence',\n"," 'max_num_adverbs_in_paragraph',\n"," 'num_nouns_in_essay',\n"," 'min_num_pronouns_in_paragraph',\n"," '25th_percentile_num_verbs_in_paragraph',\n"," '75th_percentile_num_adverbs_in_sentence',\n"," '75th_percentile_num_adjectives_in_paragraph',\n"," 'min_num_adjectives_in_paragraph']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["input_dir = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2\"\n","test_data = pd.read_csv(os.path.join(input_dir, 'test.csv'))\n","\n","print(test_data.shape)\n","\n","data_dir = \"/kaggle/input/aes-linguistic\"\n","with open(os.path.join(data_dir, 'features.txt'), 'r') as f:\n","    FEATURES = f.read().splitlines()\n","\n","FEATURES = sorted(list(set(FEATURES)))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:54:21.703558Z","iopub.status.busy":"2024-05-14T15:54:21.702662Z","iopub.status.idle":"2024-05-14T15:54:21.710943Z","shell.execute_reply":"2024-05-14T15:54:21.709836Z","shell.execute_reply.started":"2024-05-14T15:54:21.703524Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text: str):\n","    text = text.lower()\n","    # text = removeHTML(text)\n","    text = re.sub(\"http\\w+\", '', text)  # remove urls\n","    text = re.sub(r\"\\s+\", \" \", text)  # remove extra spaces\n","#     x = expandContractions(x)\n","    text = re.sub(r\"\\.+\", \".\", text)  # remove extra periods\n","    text = re.sub(r\"\\,+\", \",\", text)  # remove extra commas\n","    text = text.strip()  # remove leading and trailing spaces\n","    return text"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:54:23.312310Z","iopub.status.busy":"2024-05-14T15:54:23.311681Z","iopub.status.idle":"2024-05-14T15:54:23.316637Z","shell.execute_reply":"2024-05-14T15:54:23.315795Z","shell.execute_reply.started":"2024-05-14T15:54:23.312279Z"},"trusted":true},"outputs":[],"source":["def is_misspelled(words: list):\n","    return len([spell.unknown(word) for word in words])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:54:24.368202Z","iopub.status.busy":"2024-05-14T15:54:24.367334Z","iopub.status.idle":"2024-05-14T15:54:24.378991Z","shell.execute_reply":"2024-05-14T15:54:24.377942Z","shell.execute_reply.started":"2024-05-14T15:54:24.368170Z"},"trusted":true},"outputs":[],"source":["def get_paragraphs(data_df: pd.DataFrame):\n","    data_df['paragraph'] = data_df['full_text'].apply(\n","        lambda x: x.split(\"\\n\\n\"))\n","\n","    # preprocess paragraphs\n","    data_df['paragraph'] = data_df['paragraph'].apply(\n","        lambda x: [preprocess_text(para) for para in x])\n","\n","    # drop empty paragraphs\n","    data_df['paragraph'] = data_df['paragraph'].apply(\n","        lambda x: [para for para in x if para.strip()])\n","\n","    return data_df\n","\n","\n","def get_sentences(data_df: pd.DataFrame):\n","    # nlp.add_pipe('sentencizer')\n","    if 'sentencizer' not in nlp.pipe_names:\n","        nlp.add_pipe('sentencizer')\n","    data_df['sentence'] = data_df['paragraph'].apply(\n","        lambda x: [i.sent for i in nlp(x).sents])\n","    return data_df\n","\n","\n","def get_tokens(data_df: pd.DataFrame):\n","    data_df['words'] = data_df['sentence'].apply(\n","        lambda x: [word.text for word in x if word.text])\n","    data_df['lemmas'] = data_df['sentence'].apply(\n","        lambda x: [word.lemma_ for word in x if word.text])\n","    data_df['pos'] = data_df['sentence'].apply(\n","        lambda x: [word.pos_ for word in x if word.text])\n","    data_df['is_stop'] = data_df['sentence'].apply(\n","        lambda x: [word.is_stop for word in x if word.text])\n","\n","    return data_df"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:54:25.831134Z","iopub.status.busy":"2024-05-14T15:54:25.830264Z","iopub.status.idle":"2024-05-14T15:54:25.840042Z","shell.execute_reply":"2024-05-14T15:54:25.838996Z","shell.execute_reply.started":"2024-05-14T15:54:25.831090Z"},"trusted":true},"outputs":[],"source":["def get_features_in_essays(data_df: pd.DataFrame, column_name: str, feature_name: str):\n","    group = data_df.copy()\n","    new_columns = {}\n","    new_columns['mean_' + feature_name +\n","                '_in_essay'] = group[column_name].mean()\n","\n","    new_columns['max_' + feature_name +\n","                '_in_essay'] = group[column_name].max()\n","\n","    new_columns['min_' + feature_name +\n","                '_in_essay'] = group[column_name].min()\n","\n","    new_columns['25th_percentile_' + feature_name +\n","                '_in_essay'] = np.percentile(group[column_name], 25)\n","\n","    new_columns['75th_percentile_' + feature_name +\n","                '_in_essay'] = np.percentile(group[column_name], 75)\n","\n","    data_df = pd.add([data_df, pd.DataFrame(new_columns)], axis=1)\n","\n","    return data_df"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:54:28.579915Z","iopub.status.busy":"2024-05-14T15:54:28.578977Z","iopub.status.idle":"2024-05-14T15:54:28.588793Z","shell.execute_reply":"2024-05-14T15:54:28.587870Z","shell.execute_reply.started":"2024-05-14T15:54:28.579875Z"},"trusted":true},"outputs":[],"source":["def get_features_in_paragraphs(data_df: pd.DataFrame, column_name: str, feature_name: str):\n","    new_columns = {}\n","    group = data_df.copy().groupby(['essay_id'])[column_name]\n","\n","    new_columns['mean_' + feature_name +\n","                '_in_paragraph'] = group.transform('mean')\n","\n","    new_columns['max_' + feature_name +\n","                '_in_paragraph'] = group.transform('max')\n","\n","    new_columns['min_' + feature_name +\n","                '_in_paragraph'] = group.transform('min')\n","\n","    new_columns['25th_percentile_' + feature_name +\n","                '_in_paragraph'] = group.transform(lambda x: np.percentile(x, 25))\n","\n","    new_columns['75th_percentile_' + feature_name +\n","                '_in_paragraph'] = group.transform(lambda x: np.percentile(x, 75))\n","\n","    data_df = pd.concat([data_df, pd.DataFrame(new_columns)], axis=1)\n","\n","    return data_df"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:54:30.427988Z","iopub.status.busy":"2024-05-14T15:54:30.427290Z","iopub.status.idle":"2024-05-14T15:54:30.444525Z","shell.execute_reply":"2024-05-14T15:54:30.443567Z","shell.execute_reply.started":"2024-05-14T15:54:30.427934Z"},"trusted":true},"outputs":[],"source":["def get_features_in_sentences(data_df: pd.DataFrame, column_name: str, feature_name: str):\n","    new_columns = {}\n","    group = data_df.copy().groupby(['essay_id'])[column_name]\n","\n","    new_columns['mean_' + feature_name +\n","                '_in_sentence'] = group.transform('mean')\n","\n","    new_columns['max_' + feature_name +\n","                '_in_sentence'] = group.transform('max')\n","\n","    new_columns['min_' + feature_name +\n","                '_in_sentence'] = group.transform('min')\n","\n","    new_columns['25th_percentile_' + feature_name +\n","                '_in_sentence'] = group.transform(lambda x: np.percentile(x, 25))\n","\n","    new_columns['75th_percentile_' + feature_name +\n","                '_in_sentence'] = group.transform(lambda x: np.percentile(x, 75))\n","\n","    data_df = pd.concat([data_df, pd.DataFrame(new_columns)], axis=1)\n","\n","    return data_df"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:54:31.983506Z","iopub.status.busy":"2024-05-14T15:54:31.982747Z","iopub.status.idle":"2024-05-14T15:54:31.988996Z","shell.execute_reply":"2024-05-14T15:54:31.988071Z","shell.execute_reply.started":"2024-05-14T15:54:31.983474Z"},"trusted":true},"outputs":[],"source":["def get_features_multi_levels(data_df: pd.DataFrame, column_name: str, feature_name: str):\n","    data_df = get_features_in_sentences(data_df, column_name, feature_name)\n","    data_df[feature_name + '_in_paragraph'] = data_df.groupby(\n","        ['essay_id', 'paragraph'])[column_name].transform('sum')\n","    data_df = get_features_in_paragraphs(\n","        data_df, feature_name + '_in_paragraph', feature_name)\n","    data_df[feature_name +\n","            '_in_essay'] = data_df.groupby('essay_id')[column_name].transform('sum')\n","\n","    return data_df"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:55:08.839450Z","iopub.status.busy":"2024-05-14T15:55:08.838777Z","iopub.status.idle":"2024-05-14T15:55:09.330219Z","shell.execute_reply":"2024-05-14T15:55:09.329347Z","shell.execute_reply.started":"2024-05-14T15:55:08.839418Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(3, 118)\n","116\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>essay_id</th>\n","      <th>full_text</th>\n","      <th>mean_num_verbs_in_paragraph</th>\n","      <th>mean_num_pronouns_in_paragraph</th>\n","      <th>num_words_in_essay</th>\n","      <th>mean_num_verbs_in_sentence</th>\n","      <th>max_num_pronouns_in_paragraph</th>\n","      <th>max_num_proper_nouns_in_sentence</th>\n","      <th>25th_percentile_num_sentences_in_paragraph</th>\n","      <th>min_num_adverbs_in_paragraph</th>\n","      <th>...</th>\n","      <th>min_num_proper_nouns_in_sentence</th>\n","      <th>num_misspelled_words_in_essay</th>\n","      <th>mean_num_misspelled_words_in_sentence</th>\n","      <th>max_num_adverbs_in_paragraph</th>\n","      <th>num_nouns_in_essay</th>\n","      <th>min_num_pronouns_in_paragraph</th>\n","      <th>25th_percentile_num_verbs_in_paragraph</th>\n","      <th>75th_percentile_num_adverbs_in_sentence</th>\n","      <th>75th_percentile_num_adjectives_in_paragraph</th>\n","      <th>min_num_adjectives_in_paragraph</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000d118</td>\n","      <td>many people have car where they live. the thin...</td>\n","      <td>65.000000</td>\n","      <td>53.00</td>\n","      <td>546</td>\n","      <td>5.000000</td>\n","      <td>53</td>\n","      <td>5</td>\n","      <td>13.0</td>\n","      <td>15</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>546</td>\n","      <td>42.000000</td>\n","      <td>15</td>\n","      <td>107</td>\n","      <td>53</td>\n","      <td>65.0</td>\n","      <td>1.0</td>\n","      <td>38.0</td>\n","      <td>38</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000fe60</td>\n","      <td>i am a scientist at nasa that is discussing th...</td>\n","      <td>9.761905</td>\n","      <td>12.00</td>\n","      <td>373</td>\n","      <td>2.095238</td>\n","      <td>15</td>\n","      <td>2</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>373</td>\n","      <td>17.761905</td>\n","      <td>9</td>\n","      <td>54</td>\n","      <td>6</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>6.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>001ab80</td>\n","      <td>people always wish they had the same technolog...</td>\n","      <td>19.880000</td>\n","      <td>13.76</td>\n","      <td>607</td>\n","      <td>2.800000</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>4.0</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>607</td>\n","      <td>24.280000</td>\n","      <td>19</td>\n","      <td>114</td>\n","      <td>5</td>\n","      <td>15.0</td>\n","      <td>2.0</td>\n","      <td>14.0</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows Ã— 118 columns</p>\n","</div>"],"text/plain":["  essay_id                                          full_text  \\\n","0  000d118  many people have car where they live. the thin...   \n","1  000fe60  i am a scientist at nasa that is discussing th...   \n","2  001ab80  people always wish they had the same technolog...   \n","\n","   mean_num_verbs_in_paragraph  mean_num_pronouns_in_paragraph  \\\n","0                    65.000000                           53.00   \n","1                     9.761905                           12.00   \n","2                    19.880000                           13.76   \n","\n","   num_words_in_essay  mean_num_verbs_in_sentence  \\\n","0                 546                    5.000000   \n","1                 373                    2.095238   \n","2                 607                    2.800000   \n","\n","   max_num_pronouns_in_paragraph  max_num_proper_nouns_in_sentence  \\\n","0                             53                                 5   \n","1                             15                                 2   \n","2                             22                                 0   \n","\n","   25th_percentile_num_sentences_in_paragraph  min_num_adverbs_in_paragraph  \\\n","0                                        13.0                            15   \n","1                                         3.0                             0   \n","2                                         4.0                             7   \n","\n","   ...  min_num_proper_nouns_in_sentence  num_misspelled_words_in_essay  \\\n","0  ...                                 0                            546   \n","1  ...                                 0                            373   \n","2  ...                                 0                            607   \n","\n","   mean_num_misspelled_words_in_sentence  max_num_adverbs_in_paragraph  \\\n","0                              42.000000                            15   \n","1                              17.761905                             9   \n","2                              24.280000                            19   \n","\n","   num_nouns_in_essay  min_num_pronouns_in_paragraph  \\\n","0                 107                             53   \n","1                  54                              6   \n","2                 114                              5   \n","\n","   25th_percentile_num_verbs_in_paragraph  \\\n","0                                    65.0   \n","1                                    10.0   \n","2                                    15.0   \n","\n","   75th_percentile_num_adverbs_in_sentence  \\\n","0                                      1.0   \n","1                                      1.0   \n","2                                      2.0   \n","\n","   75th_percentile_num_adjectives_in_paragraph  \\\n","0                                         38.0   \n","1                                          6.0   \n","2                                         14.0   \n","\n","   min_num_adjectives_in_paragraph  \n","0                               38  \n","1                                0  \n","2                                9  \n","\n","[3 rows x 118 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["def get_features(data_df: pd.DataFrame,  save: bool = False, path: str = None):\n","    data_df = get_paragraphs(data_df).explode('paragraph')\n","\n","    data_df['full_text'] = data_df['full_text'].apply(preprocess_text)\n","\n","    data_df = get_sentences(data_df).explode('sentence')\n","\n","    data_df = get_tokens(data_df)\n","    data_df['sentence'] = data_df['sentence'].apply(lambda x: x.text)\n","\n","    # get paragraph features\n","    data_df['num_paragraphs'] = data_df.groupby(\n","        'essay_id')['paragraph'].transform('nunique')\n","\n","    # get number of sentences features\n","    data_df['num_sents_in_paragraph'] = data_df.groupby(['essay_id', 'paragraph'])[\n","        'sentence'].transform('nunique')\n","    data_df = get_features_in_paragraphs(\n","        data_df, 'num_sents_in_paragraph', 'num_sentences')\n","    \n","    data_df['num_sents_in_essay'] = data_df.groupby('essay_id')[\n","        'sentence'].transform('nunique')\n","\n","    # get number of words features\n","    data_df['num_words_in_sentence'] = data_df['words'].apply(len)\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_words_in_sentence', 'num_words')\n","\n","    # get length of words features\n","    data_df['mean_word_lens_in_sentence'] = data_df['words'].apply(\n","        lambda x: np.mean([len(word) for word in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'mean_word_lens_in_sentence', 'mean_word_lens')\n","\n","    # get number of proper nouns features\n","    data_df['num_proper_nouns_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['PROPN' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_proper_nouns_in_sentence', 'num_proper_nouns')\n","\n","    # get number of nouns features\n","    data_df['num_nouns_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['NOUN' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_nouns_in_sentence', 'num_nouns')\n","\n","    # get number of verbs features\n","    data_df['num_verbs_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['VERB' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_verbs_in_sentence', 'num_verbs')\n","\n","    # get number of adjectives features\n","    data_df['num_adjectives_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['ADJ' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_adjectives_in_sentence', 'num_adjectives')\n","\n","    # get number of adverbs features\n","    data_df['num_adverbs_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['ADV' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_adverbs_in_sentence', 'num_adverbs')\n","\n","    # get number of pronouns features\n","    data_df['num_pronouns_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['PRON' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_pronouns_in_sentence', 'num_pronouns')\n","\n","    # get number of conjunctions features\n","    data_df['num_conjunctions_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['CONJ' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_conjunctions_in_sentence', 'num_conjunctions')\n","\n","    # get number of misspelled words features\n","    data_df['num_misspelled_words_in_sentence'] = data_df['lemmas'].apply(\n","        lambda x: is_misspelled(x))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_misspelled_words_in_sentence', 'num_misspelled_words')\n","\n","    data_df = data_df[['essay_id', 'full_text', 'paragraph', 'sentence'] + FEATURES]\n","\n","    data_df = data_df.drop_duplicates()\n","\n","#     print(data_df.shape, data_df[['essay_id', 'full_text',  'score'] + list(FEATURES)].drop_duplicates().shape)\n","\n","    if save:\n","        data_df.to_csv(path, index=False)\n","        with open(os.path.join(os.path.dirname(path), 'features.txt'), 'w') as f:\n","            for item in FEATURES:\n","                f.write(\"%s\\n\" % item)\n","\n","    return data_df\n","\n","test_df = get_features(test_data)\n","\n","test_df = test_df[['essay_id', 'full_text'] + list(FEATURES)]\n","test_df = test_df.drop_duplicates()\n","\n","print(test_df.shape)\n","print(len(FEATURES))\n","\n","test_df"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:45:32.295726Z","iopub.status.busy":"2024-05-14T15:45:32.295435Z","iopub.status.idle":"2024-05-14T15:45:37.933668Z","shell.execute_reply":"2024-05-14T15:45:37.932888Z","shell.execute_reply.started":"2024-05-14T15:45:32.295701Z"},"papermill":{"duration":6.656015,"end_time":"2024-05-06T18:35:01.585699","exception":false,"start_time":"2024-05-06T18:34:54.929684","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["model_path = '/kaggle/input/essay-scoring-models/longformer-base-4096'\n","tokenizer = LongformerTokenizer.from_pretrained(model_path)\n","embedder = LongformerModel.from_pretrained(model_path, attention_window=128)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:45:37.935072Z","iopub.status.busy":"2024-05-14T15:45:37.934784Z","iopub.status.idle":"2024-05-14T15:45:37.939832Z","shell.execute_reply":"2024-05-14T15:45:37.939022Z","shell.execute_reply.started":"2024-05-14T15:45:37.935047Z"},"papermill":{"duration":0.014008,"end_time":"2024-05-06T18:35:01.605282","exception":false,"start_time":"2024-05-06T18:35:01.591274","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["hyperparameters = {\n","    'batch_size': 64,\n","    'dropout': 0.3,\n","    'test_set': {\n","        'total': len(test_df),\n","    },\n","    'linguistic_features': FEATURES,\n","    'accelator': str(device),\n","    'max_seq_len': 1929\n","}"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:49:29.137099Z","iopub.status.busy":"2024-05-14T15:49:29.136453Z","iopub.status.idle":"2024-05-14T15:49:29.161764Z","shell.execute_reply":"2024-05-14T15:49:29.160805Z","shell.execute_reply.started":"2024-05-14T15:49:29.137067Z"},"papermill":{"duration":0.028489,"end_time":"2024-05-06T18:35:01.654561","exception":false,"start_time":"2024-05-06T18:35:01.626072","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1929\n"]}],"source":["test_tokenized = tokenizer(test_df['full_text'].tolist(),\n","                           max_length=hyperparameters['max_seq_len'], \n","                           padding='max_length', truncation=True, \n","                           return_tensors=\"np\")\n","\n","test_df['input_ids'] = test_tokenized['input_ids'].tolist()\n","test_df['attention_mask'] = test_tokenized['attention_mask'].tolist()\n","\n","print(len(test_df.sample(1).iloc[0]['input_ids']))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:46:28.464134Z","iopub.status.busy":"2024-05-14T15:46:28.463770Z","iopub.status.idle":"2024-05-14T15:46:28.595404Z","shell.execute_reply":"2024-05-14T15:46:28.594416Z","shell.execute_reply.started":"2024-05-14T15:46:28.464106Z"},"papermill":{"duration":5.047432,"end_time":"2024-05-06T18:35:06.706704","exception":false,"start_time":"2024-05-06T18:35:01.659272","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[ 0.0061, -0.0151, -0.0282,  ..., -0.0039, -0.0213,  0.0058]],\n","       device='cuda:0')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["class MultiFeaturesModel(torch.nn.Module):\n","    def __init__(self, embedder,\n","                 lf_input_size, lf_hidden_size=64,\n","                 dropout=0.2):\n","        super(MultiFeaturesModel, self).__init__()\n","        # freeze\n","        for param in embedder.parameters():\n","            param.requires_grad = False\n","        # unfreeze the pooler\n","        for param in embedder.pooler.parameters():\n","            param.requires_grad = True\n","            \n","        self.embedder = embedder\n","        self.lf = torch.nn.Linear(lf_input_size,lf_hidden_size)\n","        # self.fc1 = torch.nn.Linear(lf_hidden_size + embedder.config.hidden_size, 256)\n","        # self.fc2 = torch.nn.Linear(256, 128)\n","        self.regressor = torch.nn.Linear(lf_hidden_size + embedder.config.hidden_size, 1)\n","        self.dropout = torch.nn.Dropout(dropout)\n","    \n","    def config(self):\n","        return {\n","            'embedder': self.embedder.config,\n","            'lf': {\n","                'input_size': self.lf.in_features,\n","                'hidden_size': self.lf.out_features\n","            },\n","            'regressor': {\n","                'input_size': self.regressor.in_features,\n","                'output_size': self.regressor.out_features\n","            }\n","        }\n","\n","    def forward(self, token_ids, attention_mask, ling_features):\n","        embedded = self.embedder(token_ids, attention_mask=attention_mask, output_hidden_states=True)[1]\n","        embedded = self.dropout(embedded)\n","            \n","        ling_features = self.lf(ling_features)\n","        ling_features = F.leaky_relu(ling_features)\n","        ling_features = self.dropout(ling_features)\n","            \n","        features = torch.cat((embedded, ling_features), dim=1)\n","\n","        score = self.regressor(features)\n","        return score\n","\n","model = MultiFeaturesModel(embedder, \n","                           len(FEATURES), 256,\n","                           hyperparameters['dropout'])\n","model.to(device)\n","\n","\n","model.state_dict()['regressor.weight']"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:46:37.065371Z","iopub.status.busy":"2024-05-14T15:46:37.064606Z","iopub.status.idle":"2024-05-14T15:46:37.556837Z","shell.execute_reply":"2024-05-14T15:46:37.555944Z","shell.execute_reply.started":"2024-05-14T15:46:37.065342Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[-0.0280, -0.0165,  0.0128,  ...,  0.0006, -0.0009, -0.0120]],\n","       device='cuda:0')"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load('/kaggle/input/essay-scoring-models/checkpoints/lemon-surf-61/multi_features-longformer-base-4096-15_epochs.pth',\n","                                 map_location=device))\n","\n","model.state_dict()['regressor.weight']"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:45:43.712609Z","iopub.status.busy":"2024-05-14T15:45:43.712296Z","iopub.status.idle":"2024-05-14T15:45:43.716993Z","shell.execute_reply":"2024-05-14T15:45:43.716186Z","shell.execute_reply.started":"2024-05-14T15:45:43.712583Z"},"papermill":{"duration":0.012637,"end_time":"2024-05-06T18:35:06.724962","exception":false,"start_time":"2024-05-06T18:35:06.712325","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def logit_to_score(logit, min_score=1, max_score=6):\n","    scores = torch.clamp(torch.round(logit), min_score, max_score)\n","    scores = scores.long()\n","    return scores"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T15:49:35.029956Z","iopub.status.busy":"2024-05-14T15:49:35.029356Z","iopub.status.idle":"2024-05-14T15:49:35.269479Z","shell.execute_reply":"2024-05-14T15:49:35.268530Z","shell.execute_reply.started":"2024-05-14T15:49:35.029925Z"},"papermill":{"duration":1.006252,"end_time":"2024-05-06T18:35:07.736137","exception":false,"start_time":"2024-05-06T18:35:06.729885","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-70.7109]], device='cuda:0')\n","tensor([[-33.3468]], device='cuda:0')\n","tensor([[-57.8386]], device='cuda:0')\n","(3, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>essay_id</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000d118</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000fe60</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>001ab80</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  essay_id  prediction\n","0  000d118           1\n","1  000fe60           1\n","2  001ab80           1"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["model.eval()\n","test_predictions = []\n","test_ids = []\n","\n","for i, row in test_df.iterrows():\n","    with torch.no_grad():\n","        token_ids = torch.tensor(row['input_ids']).unsqueeze(0)\n","        attention_mask = torch.tensor(row['attention_mask']).unsqueeze(0)\n","        row_ling_features = torch.tensor(row[list(FEATURES)].tolist()).unsqueeze(0)\n","\n","        output = model(token_ids.to(device), \n","                       attention_mask.to(device), \n","                       row_ling_features.to(device))\n","        print(output)\n","        test_predictions.append(output.item())\n","        test_ids.append(row['essay_id'])\n","        \n","submit_df = pd.DataFrame({\n","    'essay_id': test_ids,\n","    'prediction': logit_to_score(torch.tensor(test_predictions)).cpu().numpy()\n","})\n","print(submit_df.shape)\n","submit_df.to_csv('submission.csv', index=False)\n","\n","submit_df"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8059942,"sourceId":71485,"sourceType":"competition"},{"datasetId":4904293,"sourceId":8262511,"sourceType":"datasetVersion"},{"datasetId":4925130,"sourceId":8385416,"sourceType":"datasetVersion"},{"datasetId":4904414,"sourceId":8405782,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":68.195475,"end_time":"2024-05-06T18:35:10.595173","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-06T18:34:02.399698","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
