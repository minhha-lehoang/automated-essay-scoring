{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:34:05.173603Z","iopub.status.busy":"2024-05-06T18:34:05.173260Z","iopub.status.idle":"2024-05-06T18:34:16.234358Z","shell.execute_reply":"2024-05-06T18:34:16.233409Z"},"papermill":{"duration":11.069288,"end_time":"2024-05-06T18:34:16.236621","exception":false,"start_time":"2024-05-06T18:34:05.167333","status":"completed"},"tags":[]},"outputs":[],"source":["import random\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from string import punctuation, printable\n","import re\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","\n","import sklearn\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModel, AutoTokenizer\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","random_seed = 42\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed(random_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:34:16.246639Z","iopub.status.busy":"2024-05-06T18:34:16.246136Z","iopub.status.idle":"2024-05-06T18:34:53.635713Z","shell.execute_reply":"2024-05-06T18:34:53.634458Z"},"papermill":{"duration":37.397163,"end_time":"2024-05-06T18:34:53.638204","exception":false,"start_time":"2024-05-06T18:34:16.241041","status":"completed"},"tags":[]},"outputs":[],"source":["from distutils.dir_util import copy_tree\n","\n","copy_tree('/kaggle/input/spellchecker', '/kaggle/working/')\n","\n","!gzip '/kaggle/working/spellchecker/resources/en.json'\n","\n","!pip install '/kaggle/working/pyspellchecker-0.8.1-py3-none-any.whl'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:34:53.649381Z","iopub.status.busy":"2024-05-06T18:34:53.648700Z","iopub.status.idle":"2024-05-06T18:34:53.843818Z","shell.execute_reply":"2024-05-06T18:34:53.842996Z"},"papermill":{"duration":0.203142,"end_time":"2024-05-06T18:34:53.846134","exception":false,"start_time":"2024-05-06T18:34:53.642992","status":"completed"},"tags":[]},"outputs":[],"source":["from spellchecker import SpellChecker\n","\n","spell = SpellChecker()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:34:53.856585Z","iopub.status.busy":"2024-05-06T18:34:53.856274Z","iopub.status.idle":"2024-05-06T18:34:53.872676Z","shell.execute_reply":"2024-05-06T18:34:53.871683Z"},"papermill":{"duration":0.023856,"end_time":"2024-05-06T18:34:53.874826","exception":false,"start_time":"2024-05-06T18:34:53.850970","status":"completed"},"tags":[]},"outputs":[],"source":["input_dir = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2\"\n","test_data = pd.read_csv(os.path.join(input_dir, 'test.csv'))\n","\n","print(test_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:34:53.886487Z","iopub.status.busy":"2024-05-06T18:34:53.886195Z","iopub.status.idle":"2024-05-06T18:34:54.922653Z","shell.execute_reply":"2024-05-06T18:34:54.921897Z"},"papermill":{"duration":1.045001,"end_time":"2024-05-06T18:34:54.924890","exception":false,"start_time":"2024-05-06T18:34:53.879889","status":"completed"},"tags":[]},"outputs":[],"source":["FEATURES = []\n","\n","def preprocess_text(text: str):\n","    text = text.lower()\n","    # text = removeHTML(text)\n","    text = re.sub(\"http\\w+\", '', text)  # remove urls\n","    text = re.sub(r\"\\s+\", \" \", text)  # remove extra spaces\n","#     x = expandContractions(x)\n","    text = re.sub(r\"\\.+\", \".\", text)  # remove extra periods\n","    text = re.sub(r\"\\,+\", \",\", text)  # remove extra commas\n","    text = text.strip()  # remove leading and trailing spaces\n","    return text\n","\n","def is_misspelled(words: list):\n","    return len([spell.unknown(word) for word in words])\n","\n","\n","def get_paragraphs(data_df: pd.DataFrame):\n","    data_df['paragraph'] = data_df['full_text'].apply(\n","        lambda x: x.split(\"\\n\\n\"))\n","\n","    # preprocess paragraphs\n","    data_df['paragraph'] = data_df['paragraph'].apply(\n","        lambda x: [preprocess_text(para) for para in x])\n","\n","    # drop empty paragraphs\n","    data_df['paragraph'] = data_df['paragraph'].apply(\n","        lambda x: [para for para in x if para.strip()])\n","\n","    return data_df\n","\n","def get_sentences(data_df: pd.DataFrame):\n","    # nlp.add_pipe('sentencizer')\n","    if 'sentencizer' not in nlp.pipe_names:\n","        nlp.add_pipe('sentencizer')\n","    data_df['sentence'] = data_df['paragraph'].apply(\n","        lambda x: [i.sent for i in nlp(x).sents])\n","    return data_df\n","\n","\n","def get_tokens(data_df: pd.DataFrame):\n","    data_df['words'] = data_df['sentence'].apply(\n","        lambda x: [word.text for word in x if word.text])\n","    data_df['lemmas'] = data_df['sentence'].apply(\n","        lambda x: [word.lemma_ for word in x if word.text])\n","    data_df['pos'] = data_df['sentence'].apply(\n","        lambda x: [word.pos_ for word in x if word.text])\n","    data_df['is_stop'] = data_df['sentence'].apply(\n","        lambda x: [word.is_stop for word in x if word.text])\n","\n","    return data_df\n","\n","def get_features_in_essays(data_df: pd.DataFrame, column_name: str, feature_name: str):\n","    new_columns = {}\n","    new_columns['mean_' + feature_name +\n","                '_in_essay'] = data_df.copy()[column_name].mean()\n","    FEATURES.append('mean_' + feature_name + '_in_essay')\n","\n","    new_columns['max_' + feature_name +\n","                '_in_essay'] = data_df[column_name].max()\n","    FEATURES.append('max_' + feature_name + '_in_essay')\n","\n","    new_columns['min_' + feature_name +\n","                '_in_essay'] = data_df[column_name].min()\n","    FEATURES.append('min_' + feature_name + '_in_essay')\n","\n","    new_columns['25th_percentile_' + feature_name +\n","                '_in_essay'] = np.percentile(data_df[column_name], 25)\n","    FEATURES.append('25th_percentile_' + feature_name + '_in_essay')\n","\n","    new_columns['75th_percentile_' + feature_name +\n","                '_in_essay'] = np.percentile(data_df[column_name], 75)\n","    FEATURES.append('75th_percentile_' + feature_name + '_in_essay')\n","\n","    data_df = pd.concat([data_df, pd.DataFrame(new_columns)], axis=1)\n","\n","    return data_df\n","\n","\n","def get_features_in_paragraphs(data_df: pd.DataFrame, column_name: str, feature_name: str):\n","    new_columns = {}\n","    group = data_df.copy().groupby(['essay_id'])[column_name]\n","\n","    new_columns['mean_' + feature_name +\n","                '_in_paragraph'] = group.transform('mean')\n","    FEATURES.append('mean_' + feature_name + '_in_paragraph')\n","\n","    new_columns['max_' + feature_name +\n","                '_in_paragraph'] = group.transform('max')\n","    FEATURES.append('max_' + feature_name + '_in_paragraph')\n","\n","    new_columns['min_' + feature_name +\n","                '_in_paragraph'] = group.transform('min')\n","    FEATURES.append('min_' + feature_name + '_in_paragraph')\n","\n","    new_columns['25th_percentile_' + feature_name +\n","                '_in_paragraph'] = group.transform(lambda x: np.percentile(x, 25))\n","    FEATURES.append('25th_percentile_' + feature_name + '_in_paragraph')\n","\n","    new_columns['75th_percentile_' + feature_name +\n","                '_in_paragraph'] = group.transform(lambda x: np.percentile(x, 75))\n","    FEATURES.append('75th_percentile_' + feature_name + '_in_paragraph')\n","\n","    data_df = pd.concat([data_df, pd.DataFrame(new_columns)], axis=1)\n","\n","    return data_df\n","\n","\n","def get_features_in_sentences(data_df: pd.DataFrame, column_name: str, feature_name: str):\n","    new_columns = {}\n","    group = data_df.copy().groupby(['essay_id'])[column_name]\n","\n","    new_columns['mean_' + feature_name +\n","                '_in_sentence'] = group.transform('mean')\n","    FEATURES.append('mean_' + feature_name + '_in_sentence')\n","\n","    new_columns['max_' + feature_name +\n","                '_in_sentence'] = group.transform('max')\n","    FEATURES.append('max_' + feature_name + '_in_sentence')\n","\n","    new_columns['min_' + feature_name +\n","                '_in_sentence'] = group.transform('min')\n","    FEATURES.append('min_' + feature_name + '_in_sentence')\n","\n","    new_columns['25th_percentile_' + feature_name +\n","                '_in_sentence'] = group.transform(lambda x: np.percentile(x, 25))\n","    FEATURES.append('25th_percentile_' + feature_name + '_in_sentence')\n","\n","    new_columns['75th_percentile_' + feature_name +\n","                '_in_sentence'] = group.transform(lambda x: np.percentile(x, 75))\n","    FEATURES.append('75th_percentile_' + feature_name + '_in_sentence')\n","\n","    data_df = pd.concat([data_df, pd.DataFrame(new_columns)], axis=1)\n","\n","    return data_df\n","\n","def get_features_multi_levels(data_df: pd.DataFrame, column_name: str, feature_name: str):\n","    data_df = get_features_in_sentences(data_df, column_name, feature_name)\n","    data_df[feature_name + '_in_paragraph'] = data_df.groupby(\n","        ['essay_id', 'paragraph'])[column_name].transform('sum')\n","    data_df = get_features_in_paragraphs(\n","        data_df, feature_name + '_in_paragraph', feature_name)\n","    data_df[feature_name +\n","            '_in_essay'] = data_df.groupby('essay_id')[column_name].transform('sum')\n","    FEATURES.append(feature_name + '_in_essay')\n","\n","    return data_df\n","\n","\n","def get_features(data_df: pd.DataFrame):\n","    data_df = get_paragraphs(data_df).explode('paragraph')\n","\n","    data_df['full_text'] = data_df['full_text'].apply(preprocess_text)\n","\n","    data_df = get_sentences(data_df).explode('sentence')\n","\n","    data_df = get_tokens(data_df)\n","    data_df['sentence'] = data_df['sentence'].apply(lambda x: x.text)\n","\n","    # get paragraph features\n","    data_df['num_paragraphs'] = data_df.groupby(\n","        'essay_id')['paragraph'].transform('nunique')\n","    FEATURES.append('num_paragraphs')\n","\n","    # get number of sentences features\n","    data_df['num_sents_in_paragraph'] = data_df.groupby(['essay_id', 'paragraph'])[\n","        'sentence'].transform('nunique')\n","    data_df = get_features_in_paragraphs(\n","        data_df, 'num_sents_in_paragraph', 'num_sentences')\n","    \n","    data_df['num_sents_in_essay'] = data_df.groupby('essay_id')[\n","        'sentence'].transform('nunique')\n","\n","    # get number of words features\n","    data_df['num_words_in_sentence'] = data_df['words'].apply(len)\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_words_in_sentence', 'num_words')\n","\n","    # get length of words features\n","    data_df['mean_word_lens_in_sentence'] = data_df['words'].apply(\n","        lambda x: np.mean([len(word) for word in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'mean_word_lens_in_sentence', 'mean_word_lens')\n","\n","    # get number of proper nouns features\n","    data_df['num_proper_nouns_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['PROPN' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_proper_nouns_in_sentence', 'num_proper_nouns')\n","\n","    # get number of nouns features\n","    data_df['num_nouns_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['NOUN' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_nouns_in_sentence', 'num_nouns')\n","\n","    # get number of verbs features\n","    data_df['num_verbs_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['VERB' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_verbs_in_sentence', 'num_verbs')\n","\n","    # get number of adjectives features\n","    data_df['num_adjectives_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['ADJ' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_adjectives_in_sentence', 'num_adjectives')\n","\n","    # get number of adverbs features\n","    data_df['num_adverbs_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['ADV' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_adverbs_in_sentence', 'num_adverbs')\n","\n","    # get number of pronouns features\n","    data_df['num_pronouns_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['PRON' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_pronouns_in_sentence', 'num_pronouns')\n","\n","    # get number of conjunctions features\n","    data_df['num_conjunctions_in_sentence'] = data_df['pos'].apply(\n","        lambda x: np.count_nonzero(['CONJ' in pos for pos in x]))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_conjunctions_in_sentence', 'num_conjunctions')\n","\n","    # get number of misspelled words features\n","    data_df['num_misspelled_words_in_sentence'] = data_df['lemmas'].apply(\n","        lambda x: is_misspelled(x))\n","    data_df = get_features_multi_levels(\n","        data_df, 'num_misspelled_words_in_sentence', 'num_misspelled_words')\n","\n","    data_df = data_df[['essay_id', 'full_text', 'score'] + FEATURES]\n","\n","    data_df = data_df.drop_duplicates()\n","\n","    return data_df\n","\n","test_df = get_features(test_data)\n","\n","test_df = test_df.drop_duplicates()\n","test_df = test_df.loc[:,~test_df.columns.duplicated()].copy()\n","\n","test_df = test_df.reset_index(drop=True)\n","\n","print(test_df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:34:54.936032Z","iopub.status.busy":"2024-05-06T18:34:54.935237Z","iopub.status.idle":"2024-05-06T18:35:01.583156Z","shell.execute_reply":"2024-05-06T18:35:01.582077Z"},"papermill":{"duration":6.656015,"end_time":"2024-05-06T18:35:01.585699","exception":false,"start_time":"2024-05-06T18:34:54.929684","status":"completed"},"tags":[]},"outputs":[],"source":["model_path = '/kaggle/input/essay-scoring-models/longformer-base-4096'\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","embedder = AutoModel.from_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:35:01.598022Z","iopub.status.busy":"2024-05-06T18:35:01.597417Z","iopub.status.idle":"2024-05-06T18:35:01.603394Z","shell.execute_reply":"2024-05-06T18:35:01.602375Z"},"papermill":{"duration":0.014008,"end_time":"2024-05-06T18:35:01.605282","exception":false,"start_time":"2024-05-06T18:35:01.591274","status":"completed"},"tags":[]},"outputs":[],"source":["hyperparameters = {\n","    'batch_size': 64,\n","    'test_set': {\n","        'total': len(test_df),\n","    },\n","    'linguistic_features': FEATURES,\n","    'accelator': str(device),\n","    'max_seq_len': 2048\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:35:01.631720Z","iopub.status.busy":"2024-05-06T18:35:01.631450Z","iopub.status.idle":"2024-05-06T18:35:01.652708Z","shell.execute_reply":"2024-05-06T18:35:01.651744Z"},"papermill":{"duration":0.028489,"end_time":"2024-05-06T18:35:01.654561","exception":false,"start_time":"2024-05-06T18:35:01.626072","status":"completed"},"tags":[]},"outputs":[],"source":["test_tokenized = tokenizer(test_df['full_text'].tolist(),\n","                           max_length=hyperparameters['max_seq_len'], \n","                           padding='max_length', truncation=True, \n","                           return_tensors=\"np\")\n","\n","test_df['input_ids'] = test_tokenized['input_ids'].tolist()\n","test_df['attention_mask'] = test_tokenized['attention_mask'].tolist()\n","\n","print(len(test_df.sample(1).iloc[0]['input_ids']))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:35:01.665115Z","iopub.status.busy":"2024-05-06T18:35:01.664838Z","iopub.status.idle":"2024-05-06T18:35:06.704671Z","shell.execute_reply":"2024-05-06T18:35:06.703538Z"},"papermill":{"duration":5.047432,"end_time":"2024-05-06T18:35:06.706704","exception":false,"start_time":"2024-05-06T18:35:01.659272","status":"completed"},"tags":[]},"outputs":[],"source":["class MultiFeaturesModel(torch.nn.Module):\n","    def __init__(self, embedder,\n","                 lf_input_size, lf_hidden_size=64,\n","                 dropout=0.2):\n","        super(MultiFeaturesModel, self).__init__()\n","        # freeze\n","        for param in embedder.parameters():\n","            param.requires_grad = False\n","        # unfreeze the pooler\n","        for param in embedder.pooler.parameters():\n","            param.requires_grad = True\n","            \n","        self.embedder = embedder\n","        self.lf = torch.nn.Linear(lf_input_size,lf_hidden_size)\n","        # self.fc1 = torch.nn.Linear(lf_hidden_size + embedder.config.hidden_size, 256)\n","        # self.fc2 = torch.nn.Linear(256, 128)\n","        self.regressor = torch.nn.Linear(lf_hidden_size + embedder.config.hidden_size, 1)\n","        self.dropout = torch.nn.Dropout(dropout)\n","    \n","    def config(self):\n","        return {\n","            'embedder': self.embedder.config,\n","            'lf': {\n","                'input_size': self.lf.in_features,\n","                'hidden_size': self.lf.out_features\n","            },\n","            'regressor': {\n","                'input_size': self.regressor.in_features,\n","                'output_size': self.regressor.out_features\n","            }\n","        }\n","\n","    def forward(self, token_ids, attention_mask, ling_features):\n","        embedded = self.embedder(token_ids, attention_mask=attention_mask, output_hidden_states=True)[1]\n","        if self.training:\n","            embedded = self.dropout(embedded)\n","            \n","        ling_features = self.lf(ling_features)\n","        ling_features = F.leaky_relu(ling_features)\n","        if self.training:\n","            ling_features = self.dropout(ling_features)\n","            \n","        features = torch.cat((embedded, ling_features), dim=1)\n","\n","        # fc1 = self.fc1(features)\n","        # fc1 = F.leaky_relu(fc1)\n","        # if self.training:\n","        #     fc1 = self.dropout(fc1)\n","        \n","        # fc2 = self.fc2(fc1)\n","        # fc2 = F.leaky_relu(fc2)\n","        # if self.training:\n","        #     fc2 = self.dropout(fc2)\n","            \n","        # score = self.regressor(fc2)\n","        score = self.regressor(features)\n","        return score\n","\n","\n","model = MultiFeaturesModel(embedder, \n","                           len(FEATURES), 128,\n","                           hyperparameters['dropout'])\n","\n","model.load_state_dict(torch.load('/kaggle/input/essay-scoring-models/hopeful-paper-52/multi_features_longformer-base-4096_2024Y-05m-10d_model.pth',\n","                                 map_location=device))\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:35:06.718768Z","iopub.status.busy":"2024-05-06T18:35:06.718436Z","iopub.status.idle":"2024-05-06T18:35:06.723123Z","shell.execute_reply":"2024-05-06T18:35:06.722309Z"},"papermill":{"duration":0.012637,"end_time":"2024-05-06T18:35:06.724962","exception":false,"start_time":"2024-05-06T18:35:06.712325","status":"completed"},"tags":[]},"outputs":[],"source":["def logit_to_score(logit, min_score=1, max_score=6):\n","    scores = torch.clamp(torch.round(logit), min_score, max_score)\n","    scores = scores.long()\n","    return scores"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:35:06.735883Z","iopub.status.busy":"2024-05-06T18:35:06.735583Z","iopub.status.idle":"2024-05-06T18:35:07.733887Z","shell.execute_reply":"2024-05-06T18:35:07.732675Z"},"papermill":{"duration":1.006252,"end_time":"2024-05-06T18:35:07.736137","exception":false,"start_time":"2024-05-06T18:35:06.729885","status":"completed"},"tags":[]},"outputs":[],"source":["model.eval()\n","test_predictions = []\n","\n","with torch.no_grad():\n","    for i, row in test_df.iterrows():\n","        token_ids = torch.tensor(row['input_ids']).unsqueeze(0)\n","        attention_mask = torch.tensor(row['attention_mask']).unsqueeze(0)\n","        row_ling_features = torch.tensor(row[FEATURES].tolist()).unsqueeze(0)\n","\n","        output = model(token_ids.to(device), \n","                       attention_mask.to(device), \n","                       row_ling_features.to(device))\n","        test_predictions.append(output.item())\n","        \n","submit_df = pd.DataFrame({\n","    'essay_id': test_df['essay_id'],\n","    'prediction': logit_to_score(torch.tensor(test_predictions)).cpu().numpy()\n","})\n","print(submit_df.shape)\n","submit_df.to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8059942,"sourceId":71485,"sourceType":"competition"},{"datasetId":4904293,"sourceId":8262511,"sourceType":"datasetVersion"},{"datasetId":4904414,"sourceId":8375844,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":68.195475,"end_time":"2024-05-06T18:35:10.595173","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-06T18:34:02.399698","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
