{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d3f3dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:15.361640Z",
     "iopub.status.busy": "2024-05-11T14:51:15.361045Z",
     "iopub.status.idle": "2024-05-11T14:51:28.698602Z",
     "shell.execute_reply": "2024-05-11T14:51:28.697372Z"
    },
    "papermill": {
     "duration": 13.346552,
     "end_time": "2024-05-11T14:51:28.701235",
     "exception": false,
     "start_time": "2024-05-11T14:51:15.354683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\r\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.8.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a0cec1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:28.712683Z",
     "iopub.status.busy": "2024-05-11T14:51:28.712287Z",
     "iopub.status.idle": "2024-05-11T14:51:35.242536Z",
     "shell.execute_reply": "2024-05-11T14:51:35.241465Z"
    },
    "papermill": {
     "duration": 6.538901,
     "end_time": "2024-05-11T14:51:35.245133",
     "exception": false,
     "start_time": "2024-05-11T14:51:28.706232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation, printable\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "spell = SpellChecker()\n",
    "# corpus = set(nlp.vocab.strings)\n",
    "\n",
    "FEATURES = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21a063f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.256455Z",
     "iopub.status.busy": "2024-05-11T14:51:35.255875Z",
     "iopub.status.idle": "2024-05-11T14:51:35.261990Z",
     "shell.execute_reply": "2024-05-11T14:51:35.261093Z"
    },
    "papermill": {
     "duration": 0.014185,
     "end_time": "2024-05-11T14:51:35.264155",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.249970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text: str):\n",
    "    text = text.lower()\n",
    "    # text = removeHTML(text)\n",
    "    text = re.sub(\"http\\w+\", '', text)  # remove urls\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # remove extra spaces\n",
    "#     x = expandContractions(x)\n",
    "    text = re.sub(r\"\\.+\", \".\", text)  # remove extra periods\n",
    "    text = re.sub(r\"\\,+\", \",\", text)  # remove extra commas\n",
    "    text = text.strip()  # remove leading and trailing spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c520efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.293459Z",
     "iopub.status.busy": "2024-05-11T14:51:35.293095Z",
     "iopub.status.idle": "2024-05-11T14:51:35.297842Z",
     "shell.execute_reply": "2024-05-11T14:51:35.296983Z"
    },
    "papermill": {
     "duration": 0.012694,
     "end_time": "2024-05-11T14:51:35.300044",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.287350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_misspelled(words: list):\n",
    "    return len([spell.unknown(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d72901d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.311070Z",
     "iopub.status.busy": "2024-05-11T14:51:35.310709Z",
     "iopub.status.idle": "2024-05-11T14:51:35.320341Z",
     "shell.execute_reply": "2024-05-11T14:51:35.319442Z"
    },
    "papermill": {
     "duration": 0.017726,
     "end_time": "2024-05-11T14:51:35.322508",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.304782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_paragraphs(data_df: pd.DataFrame):\n",
    "    data_df['paragraph'] = data_df['full_text'].apply(\n",
    "        lambda x: x.split(\"\\n\\n\"))\n",
    "\n",
    "    # preprocess paragraphs\n",
    "    data_df['paragraph'] = data_df['paragraph'].apply(\n",
    "        lambda x: [preprocess_text(para) for para in x])\n",
    "\n",
    "    # drop empty paragraphs\n",
    "    data_df['paragraph'] = data_df['paragraph'].apply(\n",
    "        lambda x: [para for para in x if para.strip()])\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_sentences(data_df: pd.DataFrame):\n",
    "    # nlp.add_pipe('sentencizer')\n",
    "    if 'sentencizer' not in nlp.pipe_names:\n",
    "        nlp.add_pipe('sentencizer')\n",
    "    data_df['sentence'] = data_df['paragraph'].apply(\n",
    "        lambda x: [i.sent for i in nlp(x).sents])\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_tokens(data_df: pd.DataFrame):\n",
    "    data_df['words'] = data_df['sentence'].apply(\n",
    "        lambda x: [word.text for word in x if word.text])\n",
    "    data_df['lemmas'] = data_df['sentence'].apply(\n",
    "        lambda x: [word.lemma_ for word in x if word.text])\n",
    "    data_df['pos'] = data_df['sentence'].apply(\n",
    "        lambda x: [word.pos_ for word in x if word.text])\n",
    "    data_df['is_stop'] = data_df['sentence'].apply(\n",
    "        lambda x: [word.is_stop for word in x if word.text])\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f81a0d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.333370Z",
     "iopub.status.busy": "2024-05-11T14:51:35.333011Z",
     "iopub.status.idle": "2024-05-11T14:51:35.340597Z",
     "shell.execute_reply": "2024-05-11T14:51:35.339618Z"
    },
    "papermill": {
     "duration": 0.015585,
     "end_time": "2024-05-11T14:51:35.342761",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.327176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features_in_essays(data_df: pd.DataFrame, column_name: str, feature_name: str):\n",
    "    group = data_df.copy()\n",
    "    new_columns = {}\n",
    "    new_columns['mean_' + feature_name +\n",
    "                '_in_essay'] = group[column_name].mean()\n",
    "    FEATURES.add('mean_' + feature_name + '_in_essay')\n",
    "\n",
    "    new_columns['max_' + feature_name +\n",
    "                '_in_essay'] = group[column_name].max()\n",
    "    FEATURES.add('max_' + feature_name + '_in_essay')\n",
    "\n",
    "    new_columns['min_' + feature_name +\n",
    "                '_in_essay'] = group[column_name].min()\n",
    "    FEATURES.add('min_' + feature_name + '_in_essay')\n",
    "\n",
    "    new_columns['25th_percentile_' + feature_name +\n",
    "                '_in_essay'] = np.percentile(group[column_name], 25)\n",
    "    FEATURES.add('25th_percentile_' + feature_name + '_in_essay')\n",
    "\n",
    "    new_columns['75th_percentile_' + feature_name +\n",
    "                '_in_essay'] = np.percentile(group[column_name], 75)\n",
    "    FEATURES.add('75th_percentile_' + feature_name + '_in_essay')\n",
    "\n",
    "    data_df = pd.add([data_df, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52667d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.354050Z",
     "iopub.status.busy": "2024-05-11T14:51:35.353687Z",
     "iopub.status.idle": "2024-05-11T14:51:35.361883Z",
     "shell.execute_reply": "2024-05-11T14:51:35.360829Z"
    },
    "papermill": {
     "duration": 0.016533,
     "end_time": "2024-05-11T14:51:35.364038",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.347505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features_in_paragraphs(data_df: pd.DataFrame, column_name: str, feature_name: str):\n",
    "    new_columns = {}\n",
    "    group = data_df.copy().groupby(['essay_id'])[column_name]\n",
    "\n",
    "    new_columns['mean_' + feature_name +\n",
    "                '_in_paragraph'] = group.transform('mean')\n",
    "    FEATURES.add('mean_' + feature_name + '_in_paragraph')\n",
    "\n",
    "    new_columns['max_' + feature_name +\n",
    "                '_in_paragraph'] = group.transform('max')\n",
    "    FEATURES.add('max_' + feature_name + '_in_paragraph')\n",
    "\n",
    "    new_columns['min_' + feature_name +\n",
    "                '_in_paragraph'] = group.transform('min')\n",
    "    FEATURES.add('min_' + feature_name + '_in_paragraph')\n",
    "\n",
    "    new_columns['25th_percentile_' + feature_name +\n",
    "                '_in_paragraph'] = group.transform(lambda x: np.percentile(x, 25))\n",
    "    FEATURES.add('25th_percentile_' + feature_name + '_in_paragraph')\n",
    "\n",
    "    new_columns['75th_percentile_' + feature_name +\n",
    "                '_in_paragraph'] = group.transform(lambda x: np.percentile(x, 75))\n",
    "    FEATURES.add('75th_percentile_' + feature_name + '_in_paragraph')\n",
    "\n",
    "    data_df = pd.concat([data_df, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45de5353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.374733Z",
     "iopub.status.busy": "2024-05-11T14:51:35.374370Z",
     "iopub.status.idle": "2024-05-11T14:51:35.382273Z",
     "shell.execute_reply": "2024-05-11T14:51:35.381296Z"
    },
    "papermill": {
     "duration": 0.015633,
     "end_time": "2024-05-11T14:51:35.384321",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.368688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features_in_sentences(data_df: pd.DataFrame, column_name: str, feature_name: str):\n",
    "    new_columns = {}\n",
    "    group = data_df.copy().groupby(['essay_id'])[column_name]\n",
    "\n",
    "    new_columns['mean_' + feature_name +\n",
    "                '_in_sentence'] = group.transform('mean')\n",
    "    FEATURES.add('mean_' + feature_name + '_in_sentence')\n",
    "\n",
    "    new_columns['max_' + feature_name +\n",
    "                '_in_sentence'] = group.transform('max')\n",
    "    FEATURES.add('max_' + feature_name + '_in_sentence')\n",
    "\n",
    "    new_columns['min_' + feature_name +\n",
    "                '_in_sentence'] = group.transform('min')\n",
    "    FEATURES.add('min_' + feature_name + '_in_sentence')\n",
    "\n",
    "    new_columns['25th_percentile_' + feature_name +\n",
    "                '_in_sentence'] = group.transform(lambda x: np.percentile(x, 25))\n",
    "    FEATURES.add('25th_percentile_' + feature_name + '_in_sentence')\n",
    "\n",
    "    new_columns['75th_percentile_' + feature_name +\n",
    "                '_in_sentence'] = group.transform(lambda x: np.percentile(x, 75))\n",
    "    FEATURES.add('75th_percentile_' + feature_name + '_in_sentence')\n",
    "\n",
    "    data_df = pd.concat([data_df, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dbce340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.394878Z",
     "iopub.status.busy": "2024-05-11T14:51:35.394546Z",
     "iopub.status.idle": "2024-05-11T14:51:35.400354Z",
     "shell.execute_reply": "2024-05-11T14:51:35.399447Z"
    },
    "papermill": {
     "duration": 0.013523,
     "end_time": "2024-05-11T14:51:35.402547",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.389024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features_multi_levels(data_df: pd.DataFrame, column_name: str, feature_name: str):\n",
    "    data_df = get_features_in_sentences(data_df, column_name, feature_name)\n",
    "    data_df[feature_name + '_in_paragraph'] = data_df.groupby(\n",
    "        ['essay_id', 'paragraph'])[column_name].transform('sum')\n",
    "    data_df = get_features_in_paragraphs(\n",
    "        data_df, feature_name + '_in_paragraph', feature_name)\n",
    "    data_df[feature_name +\n",
    "            '_in_essay'] = data_df.groupby('essay_id')[column_name].transform('sum')\n",
    "    FEATURES.add(feature_name + '_in_essay')\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbf6e2b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.413439Z",
     "iopub.status.busy": "2024-05-11T14:51:35.413055Z",
     "iopub.status.idle": "2024-05-11T14:51:35.433393Z",
     "shell.execute_reply": "2024-05-11T14:51:35.432335Z"
    },
    "papermill": {
     "duration": 0.028603,
     "end_time": "2024-05-11T14:51:35.435789",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.407186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(data_df: pd.DataFrame,  save: bool = False, path: str = None):\n",
    "    data_df = get_paragraphs(data_df).explode('paragraph')\n",
    "\n",
    "    data_df['full_text'] = data_df['full_text'].apply(preprocess_text)\n",
    "\n",
    "    data_df = get_sentences(data_df).explode('sentence')\n",
    "\n",
    "    data_df = get_tokens(data_df)\n",
    "    data_df['sentence'] = data_df['sentence'].apply(lambda x: x.text)\n",
    "\n",
    "    # get paragraph features\n",
    "    data_df['num_paragraphs'] = data_df.groupby(\n",
    "        'essay_id')['paragraph'].transform('nunique')\n",
    "    FEATURES.add('num_paragraphs')\n",
    "\n",
    "    # get number of sentences features\n",
    "    data_df['num_sents_in_paragraph'] = data_df.groupby(['essay_id', 'paragraph'])[\n",
    "        'sentence'].transform('nunique')\n",
    "    data_df = get_features_in_paragraphs(\n",
    "        data_df, 'num_sents_in_paragraph', 'num_sentences')\n",
    "    \n",
    "    data_df['num_sents_in_essay'] = data_df.groupby('essay_id')[\n",
    "        'sentence'].transform('nunique')\n",
    "\n",
    "    # get number of words features\n",
    "    data_df['num_words_in_sentence'] = data_df['words'].apply(len)\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'num_words_in_sentence', 'num_words')\n",
    "\n",
    "    # get length of words features\n",
    "    data_df['mean_word_lens_in_sentence'] = data_df['words'].apply(\n",
    "        lambda x: np.mean([len(word) for word in x]))\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'mean_word_lens_in_sentence', 'mean_word_lens')\n",
    "\n",
    "    # get number of proper nouns features\n",
    "    data_df['num_proper_nouns_in_sentence'] = data_df['pos'].apply(\n",
    "        lambda x: np.count_nonzero(['PROPN' in pos for pos in x]))\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'num_proper_nouns_in_sentence', 'num_proper_nouns')\n",
    "\n",
    "    # get number of nouns features\n",
    "    data_df['num_nouns_in_sentence'] = data_df['pos'].apply(\n",
    "        lambda x: np.count_nonzero(['NOUN' in pos for pos in x]))\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'num_nouns_in_sentence', 'num_nouns')\n",
    "\n",
    "    # get number of verbs features\n",
    "    data_df['num_verbs_in_sentence'] = data_df['pos'].apply(\n",
    "        lambda x: np.count_nonzero(['VERB' in pos for pos in x]))\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'num_verbs_in_sentence', 'num_verbs')\n",
    "\n",
    "    # get number of adjectives features\n",
    "    data_df['num_adjectives_in_sentence'] = data_df['pos'].apply(\n",
    "        lambda x: np.count_nonzero(['ADJ' in pos for pos in x]))\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'num_adjectives_in_sentence', 'num_adjectives')\n",
    "\n",
    "    # get number of adverbs features\n",
    "    data_df['num_adverbs_in_sentence'] = data_df['pos'].apply(\n",
    "        lambda x: np.count_nonzero(['ADV' in pos for pos in x]))\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'num_adverbs_in_sentence', 'num_adverbs')\n",
    "\n",
    "    # get number of pronouns features\n",
    "    data_df['num_pronouns_in_sentence'] = data_df['pos'].apply(\n",
    "        lambda x: np.count_nonzero(['PRON' in pos for pos in x]))\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'num_pronouns_in_sentence', 'num_pronouns')\n",
    "\n",
    "    # get number of conjunctions features\n",
    "    data_df['num_conjunctions_in_sentence'] = data_df['pos'].apply(\n",
    "        lambda x: np.count_nonzero(['CONJ' in pos for pos in x]))\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'num_conjunctions_in_sentence', 'num_conjunctions')\n",
    "\n",
    "    # get number of misspelled words features\n",
    "    data_df['num_misspelled_words_in_sentence'] = data_df['lemmas'].apply(\n",
    "        lambda x: is_misspelled(x))\n",
    "    data_df = get_features_multi_levels(\n",
    "        data_df, 'num_misspelled_words_in_sentence', 'num_misspelled_words')\n",
    "\n",
    "    data_df = data_df[['essay_id', 'full_text',\n",
    "                       'score', 'paragraph', 'sentence'] + list(FEATURES)]\n",
    "\n",
    "    data_df = data_df.drop_duplicates()\n",
    "\n",
    "#     print(data_df.shape, data_df[['essay_id', 'full_text',  'score'] + list(FEATURES)].drop_duplicates().shape)\n",
    "\n",
    "    if save:\n",
    "        data_df.to_csv(path, index=False)\n",
    "        with open(os.path.join(os.path.dirname(path), 'features.txt'), 'w') as f:\n",
    "            for item in FEATURES:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "    return data_df, FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8233ae90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.447121Z",
     "iopub.status.busy": "2024-05-11T14:51:35.446187Z",
     "iopub.status.idle": "2024-05-11T14:51:35.451010Z",
     "shell.execute_reply": "2024-05-11T14:51:35.450073Z"
    },
    "papermill": {
     "duration": 0.01256,
     "end_time": "2024-05-11T14:51:35.453070",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.440510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # train_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
    "\n",
    "# # get_features(train_df, True, os.path.join('/kaggle/working', 'train_linguistic.csv'))\n",
    "\n",
    "# temp_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv', nrows=10)\n",
    "\n",
    "# feat_df, feat = get_features(temp_df)\n",
    "\n",
    "# print(len(feat))\n",
    "\n",
    "# feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe27bab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T14:51:35.465318Z",
     "iopub.status.busy": "2024-05-11T14:51:35.464454Z",
     "iopub.status.idle": "2024-05-11T15:22:28.645488Z",
     "shell.execute_reply": "2024-05-11T15:22:28.644455Z"
    },
    "papermill": {
     "duration": 1853.195732,
     "end_time": "2024-05-11T15:22:28.653482",
     "exception": false,
     "start_time": "2024-05-11T14:51:35.457750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      essay_id                                          full_text  score  \\\n",
       " 0      000d118  many people have car where they live. the thin...      3   \n",
       " 0      000d118  many people have car where they live. the thin...      3   \n",
       " 0      000d118  many people have car where they live. the thin...      3   \n",
       " 0      000d118  many people have car where they live. the thin...      3   \n",
       " 0      000d118  many people have car where they live. the thin...      3   \n",
       " ...        ...                                                ...    ...   \n",
       " 17306  fffed3e  venus is worthy place to study but dangerous. ...      2   \n",
       " 17306  fffed3e  venus is worthy place to study but dangerous. ...      2   \n",
       " 17306  fffed3e  venus is worthy place to study but dangerous. ...      2   \n",
       " 17306  fffed3e  venus is worthy place to study but dangerous. ...      2   \n",
       " 17306  fffed3e  venus is worthy place to study but dangerous. ...      2   \n",
       " \n",
       "                                                paragraph  \\\n",
       " 0      many people have car where they live. the thin...   \n",
       " 0      many people have car where they live. the thin...   \n",
       " 0      many people have car where they live. the thin...   \n",
       " 0      many people have car where they live. the thin...   \n",
       " 0      many people have car where they live. the thin...   \n",
       " ...                                                  ...   \n",
       " 17306                                     of the planet.   \n",
       " 17306  just like when they sent a robot to the planet...   \n",
       " 17306  just like when they sent a robot to the planet...   \n",
       " 17306  just like when they sent a robot to the planet...   \n",
       " 17306  in conclusion i know that tcan do it its just ...   \n",
       " \n",
       "                                                 sentence  \\\n",
       " 0                  many people have car where they live.   \n",
       " 0      the thing they don't know is that when you use...   \n",
       " 0      street parkig ,driveways and home garages are ...   \n",
       " 0      you probaly won't see a car in vauban's street...   \n",
       " 0      the vauban people completed this in 2006 ,they...   \n",
       " ...                                                  ...   \n",
       " 17306                                     of the planet.   \n",
       " 17306  just like when they sent a robot to the planet...   \n",
       " 17306                  , but that only lasted two weeks.   \n",
       " 17306   now they are try to go for longer the two weeks.   \n",
       " 17306  in conclusion i know that tcan do it its just ...   \n",
       " \n",
       "        mean_num_verbs_in_paragraph  mean_num_pronouns_in_paragraph  \\\n",
       " 0                        65.000000                       53.000000   \n",
       " 0                        65.000000                       53.000000   \n",
       " 0                        65.000000                       53.000000   \n",
       " 0                        65.000000                       53.000000   \n",
       " 0                        65.000000                       53.000000   \n",
       " ...                            ...                             ...   \n",
       " 17306                    13.384615                        9.461538   \n",
       " 17306                    13.384615                        9.461538   \n",
       " 17306                    13.384615                        9.461538   \n",
       " 17306                    13.384615                        9.461538   \n",
       " 17306                    13.384615                        9.461538   \n",
       " \n",
       "        num_words_in_essay  mean_num_verbs_in_sentence  \\\n",
       " 0                     546                    5.000000   \n",
       " 0                     546                    5.000000   \n",
       " 0                     546                    5.000000   \n",
       " 0                     546                    5.000000   \n",
       " 0                     546                    5.000000   \n",
       " ...                   ...                         ...   \n",
       " 17306                 169                    2.230769   \n",
       " 17306                 169                    2.230769   \n",
       " 17306                 169                    2.230769   \n",
       " 17306                 169                    2.230769   \n",
       " 17306                 169                    2.230769   \n",
       " \n",
       "        max_num_pronouns_in_paragraph  ...  min_num_proper_nouns_in_sentence  \\\n",
       " 0                                 53  ...                                 0   \n",
       " 0                                 53  ...                                 0   \n",
       " 0                                 53  ...                                 0   \n",
       " 0                                 53  ...                                 0   \n",
       " 0                                 53  ...                                 0   \n",
       " ...                              ...  ...                               ...   \n",
       " 17306                             13  ...                                 0   \n",
       " 17306                             13  ...                                 0   \n",
       " 17306                             13  ...                                 0   \n",
       " 17306                             13  ...                                 0   \n",
       " 17306                             13  ...                                 0   \n",
       " \n",
       "        num_misspelled_words_in_essay  mean_num_misspelled_words_in_sentence  \\\n",
       " 0                                546                                   42.0   \n",
       " 0                                546                                   42.0   \n",
       " 0                                546                                   42.0   \n",
       " 0                                546                                   42.0   \n",
       " 0                                546                                   42.0   \n",
       " ...                              ...                                    ...   \n",
       " 17306                            169                                   13.0   \n",
       " 17306                            169                                   13.0   \n",
       " 17306                            169                                   13.0   \n",
       " 17306                            169                                   13.0   \n",
       " 17306                            169                                   13.0   \n",
       " \n",
       "        max_num_adverbs_in_paragraph  num_nouns_in_essay  \\\n",
       " 0                                15                 107   \n",
       " 0                                15                 107   \n",
       " 0                                15                 107   \n",
       " 0                                15                 107   \n",
       " 0                                15                 107   \n",
       " ...                             ...                 ...   \n",
       " 17306                             7                  19   \n",
       " 17306                             7                  19   \n",
       " 17306                             7                  19   \n",
       " 17306                             7                  19   \n",
       " 17306                             7                  19   \n",
       " \n",
       "        min_num_pronouns_in_paragraph  25th_percentile_num_verbs_in_paragraph  \\\n",
       " 0                                 53                                    65.0   \n",
       " 0                                 53                                    65.0   \n",
       " 0                                 53                                    65.0   \n",
       " 0                                 53                                    65.0   \n",
       " 0                                 53                                    65.0   \n",
       " ...                              ...                                     ...   \n",
       " 17306                              0                                     6.0   \n",
       " 17306                              0                                     6.0   \n",
       " 17306                              0                                     6.0   \n",
       " 17306                              0                                     6.0   \n",
       " 17306                              0                                     6.0   \n",
       " \n",
       "        75th_percentile_num_adverbs_in_sentence  \\\n",
       " 0                                          1.0   \n",
       " 0                                          1.0   \n",
       " 0                                          1.0   \n",
       " 0                                          1.0   \n",
       " 0                                          1.0   \n",
       " ...                                        ...   \n",
       " 17306                                      1.0   \n",
       " 17306                                      1.0   \n",
       " 17306                                      1.0   \n",
       " 17306                                      1.0   \n",
       " 17306                                      1.0   \n",
       " \n",
       "        75th_percentile_num_adjectives_in_paragraph  \\\n",
       " 0                                             38.0   \n",
       " 0                                             38.0   \n",
       " 0                                             38.0   \n",
       " 0                                             38.0   \n",
       " 0                                             38.0   \n",
       " ...                                            ...   \n",
       " 17306                                          4.0   \n",
       " 17306                                          4.0   \n",
       " 17306                                          4.0   \n",
       " 17306                                          4.0   \n",
       " 17306                                          4.0   \n",
       " \n",
       "        min_num_adjectives_in_paragraph  \n",
       " 0                                   38  \n",
       " 0                                   38  \n",
       " 0                                   38  \n",
       " 0                                   38  \n",
       " 0                                   38  \n",
       " ...                                ...  \n",
       " 17306                                0  \n",
       " 17306                                0  \n",
       " 17306                                0  \n",
       " 17306                                0  \n",
       " 17306                                0  \n",
       " \n",
       " [342364 rows x 121 columns],\n",
       " {'25th_percentile_mean_word_lens_in_paragraph',\n",
       "  '25th_percentile_mean_word_lens_in_sentence',\n",
       "  '25th_percentile_num_adjectives_in_paragraph',\n",
       "  '25th_percentile_num_adjectives_in_sentence',\n",
       "  '25th_percentile_num_adverbs_in_paragraph',\n",
       "  '25th_percentile_num_adverbs_in_sentence',\n",
       "  '25th_percentile_num_conjunctions_in_paragraph',\n",
       "  '25th_percentile_num_conjunctions_in_sentence',\n",
       "  '25th_percentile_num_misspelled_words_in_paragraph',\n",
       "  '25th_percentile_num_misspelled_words_in_sentence',\n",
       "  '25th_percentile_num_nouns_in_paragraph',\n",
       "  '25th_percentile_num_nouns_in_sentence',\n",
       "  '25th_percentile_num_pronouns_in_paragraph',\n",
       "  '25th_percentile_num_pronouns_in_sentence',\n",
       "  '25th_percentile_num_proper_nouns_in_paragraph',\n",
       "  '25th_percentile_num_proper_nouns_in_sentence',\n",
       "  '25th_percentile_num_sentences_in_paragraph',\n",
       "  '25th_percentile_num_verbs_in_paragraph',\n",
       "  '25th_percentile_num_verbs_in_sentence',\n",
       "  '25th_percentile_num_words_in_paragraph',\n",
       "  '25th_percentile_num_words_in_sentence',\n",
       "  '75th_percentile_mean_word_lens_in_paragraph',\n",
       "  '75th_percentile_mean_word_lens_in_sentence',\n",
       "  '75th_percentile_num_adjectives_in_paragraph',\n",
       "  '75th_percentile_num_adjectives_in_sentence',\n",
       "  '75th_percentile_num_adverbs_in_paragraph',\n",
       "  '75th_percentile_num_adverbs_in_sentence',\n",
       "  '75th_percentile_num_conjunctions_in_paragraph',\n",
       "  '75th_percentile_num_conjunctions_in_sentence',\n",
       "  '75th_percentile_num_misspelled_words_in_paragraph',\n",
       "  '75th_percentile_num_misspelled_words_in_sentence',\n",
       "  '75th_percentile_num_nouns_in_paragraph',\n",
       "  '75th_percentile_num_nouns_in_sentence',\n",
       "  '75th_percentile_num_pronouns_in_paragraph',\n",
       "  '75th_percentile_num_pronouns_in_sentence',\n",
       "  '75th_percentile_num_proper_nouns_in_paragraph',\n",
       "  '75th_percentile_num_proper_nouns_in_sentence',\n",
       "  '75th_percentile_num_sentences_in_paragraph',\n",
       "  '75th_percentile_num_verbs_in_paragraph',\n",
       "  '75th_percentile_num_verbs_in_sentence',\n",
       "  '75th_percentile_num_words_in_paragraph',\n",
       "  '75th_percentile_num_words_in_sentence',\n",
       "  'max_mean_word_lens_in_paragraph',\n",
       "  'max_mean_word_lens_in_sentence',\n",
       "  'max_num_adjectives_in_paragraph',\n",
       "  'max_num_adjectives_in_sentence',\n",
       "  'max_num_adverbs_in_paragraph',\n",
       "  'max_num_adverbs_in_sentence',\n",
       "  'max_num_conjunctions_in_paragraph',\n",
       "  'max_num_conjunctions_in_sentence',\n",
       "  'max_num_misspelled_words_in_paragraph',\n",
       "  'max_num_misspelled_words_in_sentence',\n",
       "  'max_num_nouns_in_paragraph',\n",
       "  'max_num_nouns_in_sentence',\n",
       "  'max_num_pronouns_in_paragraph',\n",
       "  'max_num_pronouns_in_sentence',\n",
       "  'max_num_proper_nouns_in_paragraph',\n",
       "  'max_num_proper_nouns_in_sentence',\n",
       "  'max_num_sentences_in_paragraph',\n",
       "  'max_num_verbs_in_paragraph',\n",
       "  'max_num_verbs_in_sentence',\n",
       "  'max_num_words_in_paragraph',\n",
       "  'max_num_words_in_sentence',\n",
       "  'mean_mean_word_lens_in_paragraph',\n",
       "  'mean_mean_word_lens_in_sentence',\n",
       "  'mean_num_adjectives_in_paragraph',\n",
       "  'mean_num_adjectives_in_sentence',\n",
       "  'mean_num_adverbs_in_paragraph',\n",
       "  'mean_num_adverbs_in_sentence',\n",
       "  'mean_num_conjunctions_in_paragraph',\n",
       "  'mean_num_conjunctions_in_sentence',\n",
       "  'mean_num_misspelled_words_in_paragraph',\n",
       "  'mean_num_misspelled_words_in_sentence',\n",
       "  'mean_num_nouns_in_paragraph',\n",
       "  'mean_num_nouns_in_sentence',\n",
       "  'mean_num_pronouns_in_paragraph',\n",
       "  'mean_num_pronouns_in_sentence',\n",
       "  'mean_num_proper_nouns_in_paragraph',\n",
       "  'mean_num_proper_nouns_in_sentence',\n",
       "  'mean_num_sentences_in_paragraph',\n",
       "  'mean_num_verbs_in_paragraph',\n",
       "  'mean_num_verbs_in_sentence',\n",
       "  'mean_num_words_in_paragraph',\n",
       "  'mean_num_words_in_sentence',\n",
       "  'mean_word_lens_in_essay',\n",
       "  'min_mean_word_lens_in_paragraph',\n",
       "  'min_mean_word_lens_in_sentence',\n",
       "  'min_num_adjectives_in_paragraph',\n",
       "  'min_num_adjectives_in_sentence',\n",
       "  'min_num_adverbs_in_paragraph',\n",
       "  'min_num_adverbs_in_sentence',\n",
       "  'min_num_conjunctions_in_paragraph',\n",
       "  'min_num_conjunctions_in_sentence',\n",
       "  'min_num_misspelled_words_in_paragraph',\n",
       "  'min_num_misspelled_words_in_sentence',\n",
       "  'min_num_nouns_in_paragraph',\n",
       "  'min_num_nouns_in_sentence',\n",
       "  'min_num_pronouns_in_paragraph',\n",
       "  'min_num_pronouns_in_sentence',\n",
       "  'min_num_proper_nouns_in_paragraph',\n",
       "  'min_num_proper_nouns_in_sentence',\n",
       "  'min_num_sentences_in_paragraph',\n",
       "  'min_num_verbs_in_paragraph',\n",
       "  'min_num_verbs_in_sentence',\n",
       "  'min_num_words_in_paragraph',\n",
       "  'min_num_words_in_sentence',\n",
       "  'num_adjectives_in_essay',\n",
       "  'num_adverbs_in_essay',\n",
       "  'num_conjunctions_in_essay',\n",
       "  'num_misspelled_words_in_essay',\n",
       "  'num_nouns_in_essay',\n",
       "  'num_paragraphs',\n",
       "  'num_pronouns_in_essay',\n",
       "  'num_proper_nouns_in_essay',\n",
       "  'num_verbs_in_essay',\n",
       "  'num_words_in_essay'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
    "\n",
    "get_features(train_df, True, os.path.join('/kaggle/working', 'train_linguistic.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "727067d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T15:22:28.666824Z",
     "iopub.status.busy": "2024-05-11T15:22:28.666390Z",
     "iopub.status.idle": "2024-05-11T15:50:09.229748Z",
     "shell.execute_reply": "2024-05-11T15:50:09.228625Z"
    },
    "papermill": {
     "duration": 1660.584725,
     "end_time": "2024-05-11T15:50:09.244668",
     "exception": false,
     "start_time": "2024-05-11T15:22:28.659943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(           essay_id                                          full_text  score  \\\n",
       " 0      423A1CA112E2  phones modern humans today are always on their...      3   \n",
       " 0      423A1CA112E2  phones modern humans today are always on their...      3   \n",
       " 0      423A1CA112E2  phones modern humans today are always on their...      3   \n",
       " 0      423A1CA112E2  phones modern humans today are always on their...      3   \n",
       " 0      423A1CA112E2  phones modern humans today are always on their...      3   \n",
       " ...             ...                                                ...    ...   \n",
       " 25995  DF920E0A7337  have you ever asked more than one person for h...      4   \n",
       " 25995  DF920E0A7337  have you ever asked more than one person for h...      4   \n",
       " 25995  DF920E0A7337  have you ever asked more than one person for h...      4   \n",
       " 25995  DF920E0A7337  have you ever asked more than one person for h...      4   \n",
       " 25995  DF920E0A7337  have you ever asked more than one person for h...      4   \n",
       " \n",
       "                                                paragraph  \\\n",
       " 0                                                 phones   \n",
       " 0      modern humans today are always on their phone....   \n",
       " 0      modern humans today are always on their phone....   \n",
       " 0      modern humans today are always on their phone....   \n",
       " 0      modern humans today are always on their phone....   \n",
       " ...                                                  ...   \n",
       " 25995  finally, it informs you about what other peopl...   \n",
       " 25995  in conclusion, finding more than one persons v...   \n",
       " 25995  in conclusion, finding more than one persons v...   \n",
       " 25995  in conclusion, finding more than one persons v...   \n",
       " 25995  in conclusion, finding more than one persons v...   \n",
       " \n",
       "                                                 sentence  \\\n",
       " 0                                                 phones   \n",
       " 0         modern humans today are always on their phone.   \n",
       " 0      they are always on their phone more than 5 hou...   \n",
       " 0      they do is text back and forward and just have...   \n",
       " 0                         they even do it while driving.   \n",
       " ...                                                  ...   \n",
       " 25995  it all helps you on knowing not only what you ...   \n",
       " 25995  in conclusion, finding more than one persons v...   \n",
       " 25995  if you are ever interviewing people, or lookin...   \n",
       " 25995  i always ask friends, family, or other people ...   \n",
       " 25995  you've probably asked multiple people for advi...   \n",
       " \n",
       "        mean_num_verbs_in_paragraph  mean_num_pronouns_in_paragraph  \\\n",
       " 0                        12.068966                       11.896552   \n",
       " 0                        12.068966                       11.896552   \n",
       " 0                        12.068966                       11.896552   \n",
       " 0                        12.068966                       11.896552   \n",
       " 0                        12.068966                       11.896552   \n",
       " ...                            ...                             ...   \n",
       " 25995                    22.809524                       24.833333   \n",
       " 25995                    22.809524                       24.833333   \n",
       " 25995                    22.809524                       24.833333   \n",
       " 25995                    22.809524                       24.833333   \n",
       " 25995                    22.809524                       24.833333   \n",
       " \n",
       "        num_words_in_essay  mean_num_verbs_in_sentence  \\\n",
       " 0                     423                    2.137931   \n",
       " 0                     423                    2.137931   \n",
       " 0                     423                    2.137931   \n",
       " 0                     423                    2.137931   \n",
       " 0                     423                    2.137931   \n",
       " ...                   ...                         ...   \n",
       " 25995                 788                    2.452381   \n",
       " 25995                 788                    2.452381   \n",
       " 25995                 788                    2.452381   \n",
       " 25995                 788                    2.452381   \n",
       " 25995                 788                    2.452381   \n",
       " \n",
       "        max_num_pronouns_in_paragraph  ...  min_num_proper_nouns_in_sentence  \\\n",
       " 0                                 16  ...                                 0   \n",
       " 0                                 16  ...                                 0   \n",
       " 0                                 16  ...                                 0   \n",
       " 0                                 16  ...                                 0   \n",
       " 0                                 16  ...                                 0   \n",
       " ...                              ...  ...                               ...   \n",
       " 25995                             35  ...                                 0   \n",
       " 25995                             35  ...                                 0   \n",
       " 25995                             35  ...                                 0   \n",
       " 25995                             35  ...                                 0   \n",
       " 25995                             35  ...                                 0   \n",
       " \n",
       "        num_misspelled_words_in_essay  mean_num_misspelled_words_in_sentence  \\\n",
       " 0                                423                              14.586207   \n",
       " 0                                423                              14.586207   \n",
       " 0                                423                              14.586207   \n",
       " 0                                423                              14.586207   \n",
       " 0                                423                              14.586207   \n",
       " ...                              ...                                    ...   \n",
       " 25995                            788                              18.761905   \n",
       " 25995                            788                              18.761905   \n",
       " 25995                            788                              18.761905   \n",
       " 25995                            788                              18.761905   \n",
       " 25995                            788                              18.761905   \n",
       " \n",
       "        max_num_adverbs_in_paragraph  num_nouns_in_essay  \\\n",
       " 0                                 8                  69   \n",
       " 0                                 8                  69   \n",
       " 0                                 8                  69   \n",
       " 0                                 8                  69   \n",
       " 0                                 8                  69   \n",
       " ...                             ...                 ...   \n",
       " 25995                             7                 129   \n",
       " 25995                             7                 129   \n",
       " 25995                             7                 129   \n",
       " 25995                             7                 129   \n",
       " 25995                             7                 129   \n",
       " \n",
       "        min_num_pronouns_in_paragraph  25th_percentile_num_verbs_in_paragraph  \\\n",
       " 0                                  0                                     9.0   \n",
       " 0                                  0                                     9.0   \n",
       " 0                                  0                                     9.0   \n",
       " 0                                  0                                     9.0   \n",
       " 0                                  0                                     9.0   \n",
       " ...                              ...                                     ...   \n",
       " 25995                             12                                    22.0   \n",
       " 25995                             12                                    22.0   \n",
       " 25995                             12                                    22.0   \n",
       " 25995                             12                                    22.0   \n",
       " 25995                             12                                    22.0   \n",
       " \n",
       "        75th_percentile_num_adverbs_in_sentence  \\\n",
       " 0                                          1.0   \n",
       " 0                                          1.0   \n",
       " 0                                          1.0   \n",
       " 0                                          1.0   \n",
       " 0                                          1.0   \n",
       " ...                                        ...   \n",
       " 25995                                      1.0   \n",
       " 25995                                      1.0   \n",
       " 25995                                      1.0   \n",
       " 25995                                      1.0   \n",
       " 25995                                      1.0   \n",
       " \n",
       "        75th_percentile_num_adjectives_in_paragraph  \\\n",
       " 0                                              5.0   \n",
       " 0                                              5.0   \n",
       " 0                                              5.0   \n",
       " 0                                              5.0   \n",
       " 0                                              5.0   \n",
       " ...                                            ...   \n",
       " 25995                                         19.0   \n",
       " 25995                                         19.0   \n",
       " 25995                                         19.0   \n",
       " 25995                                         19.0   \n",
       " 25995                                         19.0   \n",
       " \n",
       "        min_num_adjectives_in_paragraph  \n",
       " 0                                    0  \n",
       " 0                                    0  \n",
       " 0                                    0  \n",
       " 0                                    0  \n",
       " 0                                    0  \n",
       " ...                                ...  \n",
       " 25995                               11  \n",
       " 25995                               11  \n",
       " 25995                               11  \n",
       " 25995                               11  \n",
       " 25995                               11  \n",
       " \n",
       " [296368 rows x 121 columns],\n",
       " {'25th_percentile_mean_word_lens_in_paragraph',\n",
       "  '25th_percentile_mean_word_lens_in_sentence',\n",
       "  '25th_percentile_num_adjectives_in_paragraph',\n",
       "  '25th_percentile_num_adjectives_in_sentence',\n",
       "  '25th_percentile_num_adverbs_in_paragraph',\n",
       "  '25th_percentile_num_adverbs_in_sentence',\n",
       "  '25th_percentile_num_conjunctions_in_paragraph',\n",
       "  '25th_percentile_num_conjunctions_in_sentence',\n",
       "  '25th_percentile_num_misspelled_words_in_paragraph',\n",
       "  '25th_percentile_num_misspelled_words_in_sentence',\n",
       "  '25th_percentile_num_nouns_in_paragraph',\n",
       "  '25th_percentile_num_nouns_in_sentence',\n",
       "  '25th_percentile_num_pronouns_in_paragraph',\n",
       "  '25th_percentile_num_pronouns_in_sentence',\n",
       "  '25th_percentile_num_proper_nouns_in_paragraph',\n",
       "  '25th_percentile_num_proper_nouns_in_sentence',\n",
       "  '25th_percentile_num_sentences_in_paragraph',\n",
       "  '25th_percentile_num_verbs_in_paragraph',\n",
       "  '25th_percentile_num_verbs_in_sentence',\n",
       "  '25th_percentile_num_words_in_paragraph',\n",
       "  '25th_percentile_num_words_in_sentence',\n",
       "  '75th_percentile_mean_word_lens_in_paragraph',\n",
       "  '75th_percentile_mean_word_lens_in_sentence',\n",
       "  '75th_percentile_num_adjectives_in_paragraph',\n",
       "  '75th_percentile_num_adjectives_in_sentence',\n",
       "  '75th_percentile_num_adverbs_in_paragraph',\n",
       "  '75th_percentile_num_adverbs_in_sentence',\n",
       "  '75th_percentile_num_conjunctions_in_paragraph',\n",
       "  '75th_percentile_num_conjunctions_in_sentence',\n",
       "  '75th_percentile_num_misspelled_words_in_paragraph',\n",
       "  '75th_percentile_num_misspelled_words_in_sentence',\n",
       "  '75th_percentile_num_nouns_in_paragraph',\n",
       "  '75th_percentile_num_nouns_in_sentence',\n",
       "  '75th_percentile_num_pronouns_in_paragraph',\n",
       "  '75th_percentile_num_pronouns_in_sentence',\n",
       "  '75th_percentile_num_proper_nouns_in_paragraph',\n",
       "  '75th_percentile_num_proper_nouns_in_sentence',\n",
       "  '75th_percentile_num_sentences_in_paragraph',\n",
       "  '75th_percentile_num_verbs_in_paragraph',\n",
       "  '75th_percentile_num_verbs_in_sentence',\n",
       "  '75th_percentile_num_words_in_paragraph',\n",
       "  '75th_percentile_num_words_in_sentence',\n",
       "  'max_mean_word_lens_in_paragraph',\n",
       "  'max_mean_word_lens_in_sentence',\n",
       "  'max_num_adjectives_in_paragraph',\n",
       "  'max_num_adjectives_in_sentence',\n",
       "  'max_num_adverbs_in_paragraph',\n",
       "  'max_num_adverbs_in_sentence',\n",
       "  'max_num_conjunctions_in_paragraph',\n",
       "  'max_num_conjunctions_in_sentence',\n",
       "  'max_num_misspelled_words_in_paragraph',\n",
       "  'max_num_misspelled_words_in_sentence',\n",
       "  'max_num_nouns_in_paragraph',\n",
       "  'max_num_nouns_in_sentence',\n",
       "  'max_num_pronouns_in_paragraph',\n",
       "  'max_num_pronouns_in_sentence',\n",
       "  'max_num_proper_nouns_in_paragraph',\n",
       "  'max_num_proper_nouns_in_sentence',\n",
       "  'max_num_sentences_in_paragraph',\n",
       "  'max_num_verbs_in_paragraph',\n",
       "  'max_num_verbs_in_sentence',\n",
       "  'max_num_words_in_paragraph',\n",
       "  'max_num_words_in_sentence',\n",
       "  'mean_mean_word_lens_in_paragraph',\n",
       "  'mean_mean_word_lens_in_sentence',\n",
       "  'mean_num_adjectives_in_paragraph',\n",
       "  'mean_num_adjectives_in_sentence',\n",
       "  'mean_num_adverbs_in_paragraph',\n",
       "  'mean_num_adverbs_in_sentence',\n",
       "  'mean_num_conjunctions_in_paragraph',\n",
       "  'mean_num_conjunctions_in_sentence',\n",
       "  'mean_num_misspelled_words_in_paragraph',\n",
       "  'mean_num_misspelled_words_in_sentence',\n",
       "  'mean_num_nouns_in_paragraph',\n",
       "  'mean_num_nouns_in_sentence',\n",
       "  'mean_num_pronouns_in_paragraph',\n",
       "  'mean_num_pronouns_in_sentence',\n",
       "  'mean_num_proper_nouns_in_paragraph',\n",
       "  'mean_num_proper_nouns_in_sentence',\n",
       "  'mean_num_sentences_in_paragraph',\n",
       "  'mean_num_verbs_in_paragraph',\n",
       "  'mean_num_verbs_in_sentence',\n",
       "  'mean_num_words_in_paragraph',\n",
       "  'mean_num_words_in_sentence',\n",
       "  'mean_word_lens_in_essay',\n",
       "  'min_mean_word_lens_in_paragraph',\n",
       "  'min_mean_word_lens_in_sentence',\n",
       "  'min_num_adjectives_in_paragraph',\n",
       "  'min_num_adjectives_in_sentence',\n",
       "  'min_num_adverbs_in_paragraph',\n",
       "  'min_num_adverbs_in_sentence',\n",
       "  'min_num_conjunctions_in_paragraph',\n",
       "  'min_num_conjunctions_in_sentence',\n",
       "  'min_num_misspelled_words_in_paragraph',\n",
       "  'min_num_misspelled_words_in_sentence',\n",
       "  'min_num_nouns_in_paragraph',\n",
       "  'min_num_nouns_in_sentence',\n",
       "  'min_num_pronouns_in_paragraph',\n",
       "  'min_num_pronouns_in_sentence',\n",
       "  'min_num_proper_nouns_in_paragraph',\n",
       "  'min_num_proper_nouns_in_sentence',\n",
       "  'min_num_sentences_in_paragraph',\n",
       "  'min_num_verbs_in_paragraph',\n",
       "  'min_num_verbs_in_sentence',\n",
       "  'min_num_words_in_paragraph',\n",
       "  'min_num_words_in_sentence',\n",
       "  'num_adjectives_in_essay',\n",
       "  'num_adverbs_in_essay',\n",
       "  'num_conjunctions_in_essay',\n",
       "  'num_misspelled_words_in_essay',\n",
       "  'num_nouns_in_essay',\n",
       "  'num_paragraphs',\n",
       "  'num_pronouns_in_essay',\n",
       "  'num_proper_nouns_in_essay',\n",
       "  'num_verbs_in_essay',\n",
       "  'num_words_in_essay'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_data_df = pd.read_csv('/kaggle/input/persaude-corpus-2/persuade_2.0_human_scores_demo_id_github.csv')\n",
    "\n",
    "extra_data_df = extra_data_df.rename(columns={'essay_id_comp': 'essay_id',\n",
    "                                              'holistic_essay_score': 'score'})\n",
    "\n",
    "extra_data_df = extra_data_df[['essay_id', 'full_text', 'score']]\n",
    "extra_data_df = extra_data_df[~extra_data_df['full_text'].isin(train_df['full_text'])]\n",
    "\n",
    "get_features(extra_data_df, True, os.path.join('/kaggle/working', 'extra_linguistic.csv'))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 3937250,
     "sourceId": 7017419,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3538.330708,
   "end_time": "2024-05-11T15:50:11.085619",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-11T14:51:12.754911",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
