{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Neural Network Model for Automated Essay Scoring 2.0 Kaggle Competition**\n","\n","*Version 0.2*"]},{"cell_type":"markdown","metadata":{},"source":["## Utils"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.status.busy":"2024-05-17T11:56:47.639908Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# import sys\n","# sys.path.append(\"/kaggle/input/automated-essay-scoring\")\n","\n","import random\n","import os\n","import datetime\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import cohen_kappa_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from torchinfo import summary\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from transformers import AutoTokenizer, AutoModel\n","\n","from modules.data import tokenize_text, LSCDataset, collate_fn\n","from modules.model import LSCModel\n","from modules.training import train, EarlyStopper\n","from modules.evaluate import evaluate, logit_to_score\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","random_seed = 42\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed(random_seed)\n","\n","# from kaggle_secrets import UserSecretsClient # type: ignore\n","# import wandb # type: ignore\n","\n","# user_secrets = UserSecretsClient()\n","# wandb_api = user_secrets.get_secret(\"wandb_ha\")\n","\n","# wandb.login(key=wandb_api)"]},{"cell_type":"markdown","metadata":{},"source":["## Read data and preprocess"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>essay_id</th>\n","      <th>full_text</th>\n","      <th>sentence</th>\n","      <th>score</th>\n","      <th>25th_percentile_mean_word_lens_in_paragraph</th>\n","      <th>25th_percentile_mean_word_lens_in_sentence</th>\n","      <th>25th_percentile_num_adjectives_in_paragraph</th>\n","      <th>25th_percentile_num_adjectives_in_sentence</th>\n","      <th>25th_percentile_num_adverbs_in_paragraph</th>\n","      <th>25th_percentile_num_adverbs_in_sentence</th>\n","      <th>...</th>\n","      <th>num_adjectives_in_essay</th>\n","      <th>num_adverbs_in_essay</th>\n","      <th>num_conjunctions_in_essay</th>\n","      <th>num_misspelled_words_in_essay</th>\n","      <th>num_nouns_in_essay</th>\n","      <th>num_paragraphs</th>\n","      <th>num_pronouns_in_essay</th>\n","      <th>num_proper_nouns_in_essay</th>\n","      <th>num_verbs_in_essay</th>\n","      <th>num_words_in_essay</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000d118</td>\n","      <td>many people have car where they live. the thin...</td>\n","      <td>[many people have car where they live., the th...</td>\n","      <td>3</td>\n","      <td>54.826177</td>\n","      <td>3.626506</td>\n","      <td>38.0</td>\n","      <td>2.0</td>\n","      <td>15.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>38</td>\n","      <td>15</td>\n","      <td>50</td>\n","      <td>546</td>\n","      <td>107</td>\n","      <td>1</td>\n","      <td>53</td>\n","      <td>20</td>\n","      <td>65</td>\n","      <td>546</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000fe60</td>\n","      <td>i am a scientist at nasa that is discussing th...</td>\n","      <td>[i am a scientist at nasa that is discussing t...</td>\n","      <td>3</td>\n","      <td>10.816410</td>\n","      <td>3.187500</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>12</td>\n","      <td>20</td>\n","      <td>26</td>\n","      <td>373</td>\n","      <td>54</td>\n","      <td>5</td>\n","      <td>53</td>\n","      <td>8</td>\n","      <td>44</td>\n","      <td>373</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>001ab80</td>\n","      <td>people always wish they had the same technolog...</td>\n","      <td>[people always wish they had the same technolo...</td>\n","      <td>4</td>\n","      <td>17.818487</td>\n","      <td>3.896552</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>8.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>43</td>\n","      <td>45</td>\n","      <td>35</td>\n","      <td>607</td>\n","      <td>114</td>\n","      <td>4</td>\n","      <td>49</td>\n","      <td>0</td>\n","      <td>70</td>\n","      <td>607</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>001bdc0</td>\n","      <td>we all heard about venus, the planet without a...</td>\n","      <td>[we all heard about venus, the planet without ...</td>\n","      <td>4</td>\n","      <td>15.264912</td>\n","      <td>4.033333</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>33</td>\n","      <td>19</td>\n","      <td>35</td>\n","      <td>510</td>\n","      <td>118</td>\n","      <td>5</td>\n","      <td>32</td>\n","      <td>14</td>\n","      <td>57</td>\n","      <td>510</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>002ba53</td>\n","      <td>dear, state senator this is a letter to argue ...</td>\n","      <td>[dear, state senator, this is a letter to argu...</td>\n","      <td>3</td>\n","      <td>14.339735</td>\n","      <td>3.990000</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>42</td>\n","      <td>9</td>\n","      <td>18</td>\n","      <td>419</td>\n","      <td>90</td>\n","      <td>6</td>\n","      <td>22</td>\n","      <td>6</td>\n","      <td>35</td>\n","      <td>419</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 120 columns</p>\n","</div>"],"text/plain":["  essay_id                                          full_text  \\\n","0  000d118  many people have car where they live. the thin...   \n","1  000fe60  i am a scientist at nasa that is discussing th...   \n","2  001ab80  people always wish they had the same technolog...   \n","3  001bdc0  we all heard about venus, the planet without a...   \n","4  002ba53  dear, state senator this is a letter to argue ...   \n","\n","                                            sentence  score  \\\n","0  [many people have car where they live., the th...      3   \n","1  [i am a scientist at nasa that is discussing t...      3   \n","2  [people always wish they had the same technolo...      4   \n","3  [we all heard about venus, the planet without ...      4   \n","4  [dear, state senator, this is a letter to argu...      3   \n","\n","   25th_percentile_mean_word_lens_in_paragraph  \\\n","0                                    54.826177   \n","1                                    10.816410   \n","2                                    17.818487   \n","3                                    15.264912   \n","4                                    14.339735   \n","\n","   25th_percentile_mean_word_lens_in_sentence  \\\n","0                                    3.626506   \n","1                                    3.187500   \n","2                                    3.896552   \n","3                                    4.033333   \n","4                                    3.990000   \n","\n","   25th_percentile_num_adjectives_in_paragraph  \\\n","0                                         38.0   \n","1                                          2.0   \n","2                                         10.0   \n","3                                          2.0   \n","4                                          6.0   \n","\n","   25th_percentile_num_adjectives_in_sentence  \\\n","0                                         2.0   \n","1                                         0.0   \n","2                                         1.0   \n","3                                         0.0   \n","4                                         1.0   \n","\n","   25th_percentile_num_adverbs_in_paragraph  \\\n","0                                      15.0   \n","1                                       2.0   \n","2                                       8.0   \n","3                                       4.0   \n","4                                       1.0   \n","\n","   25th_percentile_num_adverbs_in_sentence  ...  num_adjectives_in_essay  \\\n","0                                      1.0  ...                       38   \n","1                                      0.0  ...                       12   \n","2                                      1.0  ...                       43   \n","3                                      0.0  ...                       33   \n","4                                      0.0  ...                       42   \n","\n","   num_adverbs_in_essay  num_conjunctions_in_essay  \\\n","0                    15                         50   \n","1                    20                         26   \n","2                    45                         35   \n","3                    19                         35   \n","4                     9                         18   \n","\n","   num_misspelled_words_in_essay  num_nouns_in_essay  num_paragraphs  \\\n","0                            546                 107               1   \n","1                            373                  54               5   \n","2                            607                 114               4   \n","3                            510                 118               5   \n","4                            419                  90               6   \n","\n","   num_pronouns_in_essay  num_proper_nouns_in_essay  num_verbs_in_essay  \\\n","0                     53                         20                  65   \n","1                     53                          8                  44   \n","2                     49                          0                  70   \n","3                     32                         14                  57   \n","4                     22                          6                  35   \n","\n","   num_words_in_essay  \n","0                 546  \n","1                 373  \n","2                 607  \n","3                 510  \n","4                 419  \n","\n","[5 rows x 120 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# data_dir = '/kaggle/input/aes-linguistic'\n","data_dir = \"../output/\"\n","\n","with open(os.path.join(data_dir, 'features.txt'), 'r') as f:\n","    FEATURES = f.read().splitlines()\n","FEATURES = sorted(list(set(FEATURES)))\n","\n","train_data = pd.read_csv(os.path.join(data_dir, 'train_linguistic.csv'))\n","extra_data = pd.read_csv(os.path.join(data_dir, 'extra_linguistic.csv'))\n","\n","train_data = train_data.dropna(how='any')\n","extra_data = extra_data.dropna(how='any')\n","\n","train_data = train_data[['essay_id', 'full_text', 'sentence', 'score'] + FEATURES]\n","extra_data = extra_data[['essay_id', 'full_text', 'sentence', 'score'] + FEATURES]\n","\n","# group by essay_id, sentence -> list, everything else -> first\n","agg_dict = {col: 'first' for col in train_data.columns if col not in ['essay_id', 'sentence']}\n","agg_dict['sentence'] = lambda x: list(x)\n","\n","train_data = train_data.groupby('essay_id').agg(agg_dict) \n","extra_data = extra_data.groupby('essay_id').agg(agg_dict)\n","\n","train_data = train_data[['full_text', 'sentence', 'score'] + FEATURES]\n","extra_data = extra_data[['full_text', 'sentence', 'score'] + FEATURES]\n","\n","train_data = train_data.reset_index()\n","extra_data = extra_data.reset_index()\n","\n","train_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Split data into train, validation and test sets"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(24344, 120) (3043, 120) (3044, 120)\n"]}],"source":["all_data = pd.concat([train_data, extra_data], ignore_index=True)\n","\n","# shuffle the data\n","all_data = all_data.sample(frac=1, random_state=random_seed)\n","all_data = all_data.reset_index(drop=True)\n","\n","all_data = all_data[['essay_id', 'full_text', 'sentence', 'score'] + sorted(FEATURES)]\n","\n","train_ratio, val_ratio, test_ratio = 0.8, 0.1, 0.1\n","\n","train_df, val_df = train_test_split(all_data, test_size=val_ratio + test_ratio, \n","                                    random_state=random_seed)\n","val_df, test_df = train_test_split(val_df, test_size=test_ratio/(val_ratio + test_ratio), \n","                                   random_state=random_seed)\n","\n","print(train_df.shape, val_df.shape, test_df.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'train_df' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_set\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_df\u001b[49m),\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratio\u001b[39m\u001b[38;5;124m'\u001b[39m: train_ratio,\n\u001b[0;32m      9\u001b[0m     },\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_set\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(val_df),\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratio\u001b[39m\u001b[38;5;124m'\u001b[39m: val_ratio,\n\u001b[0;32m     13\u001b[0m     },\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_set\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(test_df),\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratio\u001b[39m\u001b[38;5;124m'\u001b[39m: test_ratio,\n\u001b[0;32m     17\u001b[0m     },\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinguistic_features\u001b[39m\u001b[38;5;124m'\u001b[39m: FEATURES,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelator\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(device)\n\u001b[0;32m     20\u001b[0m }\n","\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"]}],"source":["hyperparameters = {\n","    'lr': 1e-4,\n","    'dropout': 0.3,\n","    'epochs': 15,\n","    'batch_size': 6,\n","    'train_set': {\n","        'total': len(train_df),\n","        'ratio': train_ratio,\n","    },\n","    'val_set': {\n","        'total': len(val_df),\n","        'ratio': val_ratio,\n","    },\n","    'test_set': {\n","        'total': len(test_df),\n","        'ratio': test_ratio,\n","    },\n","    'linguistic_features': FEATURES,\n","    'accelator': str(device)\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## Embedding Models"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["sentence_model = \"thenlper/gte-base\"\n","sentence_tokenizer = AutoTokenizer.from_pretrained(sentence_model)\n","sentence_encoder = AutoModel.from_pretrained(sentence_model)\n","\n","# essay_model = \"/kaggle/input/essay-scoring-models/longformer-base-4096\"\n","essay_model = \"allenai/longformer-base-4096\"\n","essay_tokenizer = AutoTokenizer.from_pretrained(essay_model)\n","essay_encoder = AutoModel.from_pretrained(essay_model)"]},{"cell_type":"markdown","metadata":{},"source":["### Essay Tokenize"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["train_tokenized, hyperparameters['max_seq_len'] = tokenize_text(\n","    train_df['full_text'].tolist(), essay_tokenizer)\n","\n","train_df['essay_input_ids'] = train_tokenized['input_ids'].tolist()\n","train_df['essay_attention_mask'] = train_tokenized['attention_mask'].tolist()\n","\n","print(len(train_df.sample(1).iloc[0]['essay_input_ids']))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1929\n","1929\n"]}],"source":["val_tokenized = tokenize_text(val_df['full_text'].tolist(), essay_tokenizer, \n","                              False, hyperparameters['max_seq_len'])\n","\n","val_df['essay_input_ids'] = val_tokenized['input_ids'].tolist()\n","val_df['essay_attention_mask'] = val_tokenized['attention_mask'].tolist()\n","\n","print(len(val_df.sample(1).iloc[0]['essay_input_ids']))\n","\n","test_tokenized = tokenize_text(test_df['full_text'].tolist(), essay_tokenizer,\n","                               False, hyperparameters['max_seq_len'])\n","\n","test_df['essay_input_ids'] = test_tokenized['input_ids'].tolist()\n","test_df['essay_attention_mask'] = test_tokenized['attention_mask'].tolist()\n","\n","print(len(test_df.sample(1).iloc[0]['essay_input_ids']))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hyperparameters['max_sentence_length'] = 256"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset and DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([6, 1929])\n","torch.Size([6, 1929])\n","torch.Size([6, 116])\n","torch.Size([6, 36, 256])\n","torch.Size([6, 36, 256])\n","torch.Size([6, 1])\n"]}],"source":["train_dataset = LSCDataset(train_df, sentence_tokenizer, FEATURES,\n","                            hyperparameters['max_sentence_length'])\n","\n","val_dataset = LSCDataset(val_df, sentence_tokenizer, FEATURES,\n","                            hyperparameters['max_sentence_length'])\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'],\n","                                shuffle=True, collate_fn=collate_fn)\n","\n","val_dataloader = DataLoader(val_dataset, batch_size=hyperparameters['batch_size'],\n","                            shuffle=False, collate_fn=collate_fn)\n","\n","for features, essay_input_ids, essay_attention_mask, sent_input_ids, sent_attention_mask, score in train_dataloader:\n","    print(essay_input_ids.shape)\n","    print(essay_attention_mask.shape)\n","    print(features.shape)\n","    print(sent_input_ids.shape)\n","    print(sent_attention_mask.shape)\n","    print(score.shape)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = LSCModel(essay_encoder, sentence_encoder,\n","                 len(FEATURES), 64, dropout=hyperparameters['dropout'])\n","\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["inputs = next(iter(train_dataloader))[:-1]\n","with torch.no_grad():\n","    model_summary = summary(model, input_data=inputs)\n","print(model_summary)"]},{"cell_type":"markdown","metadata":{},"source":["## Training and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                             lr=hyperparameters['lr'],\n","                             weight_decay=1e-8)\n","scheduler = ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n","early_stopper = EarlyStopper(patience=5, min_delta=1e-4)\n","\n","train_losses, val_losses, val_kappa_scores, val_accuracies = [], [], [], []\n","\n","hyperparameters['early_stopper'] = early_stopper.__dict__['patience']\n","hyperparameters['scheduler'] = scheduler.__dict__\n","# hyperparameters['model'] = model.config()\n","hyperparameters['sentence_encoder'] = sentence_model\n","hyperparameters['essay_encoder'] = essay_model\n","\n","hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","model.to(device)\n","\n","# wandb.init(project='deep-essay-scoring',\n","#            config=hyperparameters)  # type: ignore\n","\n","print(\"Start training...\")\n","\n","for epoch in range(hyperparameters['epochs']):\n","    train_loss = train(model, optimizer, criterion, train_dataloader, is_log=False, device=device)\n","    train_losses.append(train_loss)\n","\n","    val_loss, val_scores, val_predictions = evaluate(\n","        model, criterion, val_dataloader, device=device)\n","    val_kappa = cohen_kappa_score(val_scores.cpu().numpy(),\n","                                  logit_to_score(\n","                                      val_predictions).cpu().numpy(),\n","                                  weights='quadratic')\n","\n","    val_accuracy = torch.sum(val_scores == logit_to_score(\n","        val_predictions)).float() / len(val_scores)\n","\n","    val_losses.append(val_loss)\n","    val_kappa_scores.append(val_kappa)\n","    val_accuracies.append(val_accuracy)\n","\n","    scheduler.step(val_loss)\n","\n","#     wandb.log({'train_loss': train_loss, 'val_loss': val_loss,\n","#                'val_accuracy': val_accuracy, 'val_kappa': val_kappa,\n","#                'epoch': epoch+1})\n","\n","    print(f'Epoch: {epoch+1}, Train Loss: {train_loss}, Val Loss: {val_loss}, Val Kappa: {val_kappa}, Val Accuracy: {val_accuracy}')\n","\n","#     break\n","\n","    if early_stopper.early_stop(val_losses[-1]):\n","        print(\"Early stopping\")\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_epochs = hyperparameters['epochs']\n","model_name = f\"lsc_{num_epochs}_epochs.pth\"\n","\n","# save the model\n","torch.save(model.state_dict(), model_name)\n","\n","# save the model summary\n","with open(f'model_summary_{num_epochs}_epochs.json', 'w') as f:\n","    json.dump(model_summary, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","model.eval()\n","test_predictions = []\n","\n","for i, row in test_df.iterrows():\n","    with torch.no_grad():\n","        essay_input_ids = torch.tensor(row['essay_input_ids']).unsqueeze(0)\n","        essay_attention_mask = torch.tensor(\n","            row['essay_attention_mask']).unsqueeze(0)\n","        features = torch.tensor([row[feature]\n","                                for feature in FEATURES]).unsqueeze(0)\n","        sentences = row['sentence']\n","\n","        pair_encodings = tokenize_text(sentences, sentence_tokenizer, False, hyperparameters['max_sentence_length'])\n","\n","        sent_input_ids = pad_sequence([torch.cat([pair_encoding['input_ids'] for pair_encoding in pair_encodings])],\n","                                      batch_first=True, padding_value=1)\n","        sent_attention_mask = pad_sequence([torch.cat([pair_encoding['attention_mask'] for pair_encoding in pair_encodings])],\n","                                           batch_first=True, padding_value=0)\n","\n","        output = model(essay_input_ids.to(device),\n","                       essay_attention_mask.to(device),\n","                       features.to(device),\n","                       sent_input_ids.to(device),\n","                       sent_attention_mask.to(device))\n","\n","        test_predictions.append(output.item())\n","\n","submit_df = pd.DataFrame({\n","    'essay_id': test_df['essay_id'],\n","    'score': logit_to_score(torch.tensor(test_predictions)).cpu().detach().numpy()\n","})\n","print(submit_df.shape)\n","submit_df.to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4925130,"sourceId":8385416,"sourceType":"datasetVersion"},{"datasetId":4904414,"sourceId":8436423,"sourceType":"datasetVersion"},{"datasetId":5026003,"sourceId":8439727,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
